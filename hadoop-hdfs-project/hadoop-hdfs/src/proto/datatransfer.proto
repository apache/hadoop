/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// This file contains protocol buffers that are used to transfer data
// to and from the datanode, as well as between datanodes.

option java_package = "org.apache.hadoop.hdfs.protocol.proto";
option java_outer_classname = "DataTransferProtos";
option java_generate_equals_and_hash = true;

import "hdfs.proto";

message BaseHeaderProto {
  required ExtendedBlockProto block = 1;
  optional BlockTokenIdentifierProto token = 2;
}

message ClientOperationHeaderProto {
  required BaseHeaderProto baseHeader = 1;
  required string clientName = 2;
}

message OpReadBlockProto {
  required ClientOperationHeaderProto header = 1;
  required uint64 offset = 2;
  required uint64 len = 3;
}
  
message OpWriteBlockProto {
  required ClientOperationHeaderProto header = 1;
  repeated DatanodeInfoProto targets = 2;
  optional DatanodeInfoProto source = 3;
  enum BlockConstructionStage {
    PIPELINE_SETUP_APPEND = 0;
    // pipeline set up for failed PIPELINE_SETUP_APPEND recovery
    PIPELINE_SETUP_APPEND_RECOVERY = 1;
    // data streaming
    DATA_STREAMING = 2;
    // pipeline setup for failed data streaming recovery
    PIPELINE_SETUP_STREAMING_RECOVERY = 3;
    // close the block and pipeline
    PIPELINE_CLOSE = 4;
    // Recover a failed PIPELINE_CLOSE
    PIPELINE_CLOSE_RECOVERY = 5;
    // pipeline set up for block creation
    PIPELINE_SETUP_CREATE = 6;
    // transfer RBW for adding datanodes
    TRANSFER_RBW = 7;
    // transfer Finalized for adding datanodes
    TRANSFER_FINALIZED = 8;
  }
  required BlockConstructionStage stage = 4;
  required uint32 pipelineSize = 5;
  required uint64 minBytesRcvd = 6;
  required uint64 maxBytesRcvd = 7;
  required uint64 latestGenerationStamp = 8;
}
  
message OpTransferBlockProto {
  required ClientOperationHeaderProto header = 1;
  repeated DatanodeInfoProto targets = 2;
}

message OpReplaceBlockProto {
  required BaseHeaderProto header = 1;
  required string delHint = 2;
  required DatanodeInfoProto source = 3;
}

message OpCopyBlockProto {
  required BaseHeaderProto header = 1;
}

message OpBlockChecksumProto { 
  required BaseHeaderProto header = 1;
}


message PacketHeaderProto {
  // All fields must be fixed-length!
  required sfixed64 offsetInBlock = 1;
  required sfixed64 seqno = 2;
  required bool lastPacketInBlock = 3;
  required sfixed32 dataLen = 4;
}

enum Status {
  SUCCESS = 0;
  ERROR = 1;
  ERROR_CHECKSUM = 2;
  ERROR_INVALID = 3;
  ERROR_EXISTS = 4;
  ERROR_ACCESS_TOKEN = 5;
  CHECKSUM_OK = 6;
}

message PipelineAckProto {
  required sint64 seqno = 1;
  repeated Status status = 2;
}

message BlockOpResponseProto {
  required Status status = 1;

  optional string firstBadLink = 2;
  optional OpBlockChecksumResponseProto checksumResponse = 3;

  /** explanatory text which may be useful to log on the client side */
  optional string message = 4;
}

/**
 * Message sent from the client to the DN after reading the entire
 * read request.
 */
message ClientReadStatusProto {
  required Status status = 1;
}

message DNTransferAckProto {
  required Status status = 1;
}

message OpBlockChecksumResponseProto {
  required uint32 bytesPerCrc = 1;
  required uint64 crcPerBlock = 2;
  required bytes md5 = 3;
}
