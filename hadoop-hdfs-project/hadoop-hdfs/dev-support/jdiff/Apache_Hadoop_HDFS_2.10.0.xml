<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!-- Generated by the JDiff Javadoc doclet -->
<!-- (http://www.jdiff.org) -->
<!-- on Tue Oct 29 19:51:53 PDT 2019 -->

<api
  xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'
  xsi:noNamespaceSchemaLocation='api.xsd'
  name="Apache Hadoop HDFS 2.10.0"
  jdversion="1.0.9">

<!--  Command line arguments =  -doclet org.apache.hadoop.classification.tools.IncludePublicAnnotationsJDiffDoclet -docletpath /home/jhung/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/hadoop-annotations.jar:/home/jhung/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/jdiff.jar -verbose -classpath /home/jhung/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/classes:/home/jhung/hadoop/hadoop-common-project/hadoop-annotations/target/hadoop-annotations-2.10.0.jar:/export/apps/jdk/JDK-1_7_0_51/lib/tools.jar:/home/jhung/hadoop/hadoop-common-project/hadoop-auth/target/hadoop-auth-2.10.0.jar:/home/jhung/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/jhung/.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar:/home/jhung/.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar:/home/jhung/.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar:/home/jhung/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/jhung/.m2/repository/net/minidev/json-smart/1.3.1/json-smart-1.3.1.jar:/home/jhung/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jhung/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/jhung/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/jhung/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/jhung/.m2/repository/org/apache/zookeeper/zookeeper/3.4.9/zookeeper-3.4.9.jar:/home/jhung/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/home/jhung/.m2/repository/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar:/home/jhung/hadoop/hadoop-common-project/hadoop-common/target/hadoop-common-2.10.0.jar:/home/jhung/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/jhung/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/jhung/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/jhung/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/jhung/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/home/jhung/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/jhung/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/jhung/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/jhung/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/jhung/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/jhung/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/jhung/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/jhung/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/jhung/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/home/jhung/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/home/jhung/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/jhung/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/jhung/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/home/jhung/.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/home/jhung/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/home/jhung/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/jhung/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/home/jhung/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/jhung/.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar:/home/jhung/.m2/repository/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/home/jhung/.m2/repository/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/home/jhung/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/home/jhung/.m2/repository/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar:/home/jhung/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar:/home/jhung/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar:/home/jhung/hadoop/hadoop-hdfs-project/hadoop-hdfs-client/target/hadoop-hdfs-client-2.10.0.jar:/home/jhung/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/home/jhung/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/home/jhung/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/home/jhung/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/jhung/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/jhung/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/jhung/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/jhung/.m2/repository/asm/asm/3.2/asm-3.2.jar:/home/jhung/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/jhung/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/home/jhung/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/jhung/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/jhung/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/jhung/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/jhung/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/jhung/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/jhung/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/jhung/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/jhung/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/jhung/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/jhung/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/jhung/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/home/jhung/.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/home/jhung/.m2/repository/xerces/xercesImpl/2.12.0/xercesImpl-2.12.0.jar:/home/jhung/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/jhung/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/home/jhung/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/jhung/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.7.8/jackson-databind-2.7.8.jar:/home/jhung/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.7.8/jackson-annotations-2.7.8.jar:/home/jhung/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.7.8/jackson-core-2.7.8.jar -sourcepath /home/jhung/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java -apidir /home/jhung/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/site/jdiff/xml -apiname Apache Hadoop HDFS 2.10.0 -->
<package name="org.apache.hadoop.hdfs">
  <doc>
  <![CDATA[<p>A distributed implementation of {@link
org.apache.hadoop.fs.FileSystem}.  This is loosely modelled after
Google's <a href="http://research.google.com/archive/gfs.html">GFS</a>.</p>

<p>The most important difference is that unlike GFS, Hadoop DFS files 
have strictly one writer at any one time.  Bytes are always appended 
to the end of the writer's stream.  There is no notion of "record appends"
or "mutations" that are then checked or reordered.  Writers simply emit 
a byte stream.  That byte stream is guaranteed to be stored in the 
order written.</p>]]>
  </doc>
</package>
<package name="org.apache.hadoop.hdfs.net">
</package>
<package name="org.apache.hadoop.hdfs.protocol">
</package>
<package name="org.apache.hadoop.hdfs.protocol.datatransfer">
</package>
<package name="org.apache.hadoop.hdfs.protocol.datatransfer.sasl">
</package>
<package name="org.apache.hadoop.hdfs.protocolPB">
</package>
<package name="org.apache.hadoop.hdfs.qjournal.client">
</package>
<package name="org.apache.hadoop.hdfs.qjournal.protocol">
</package>
<package name="org.apache.hadoop.hdfs.qjournal.protocolPB">
</package>
<package name="org.apache.hadoop.hdfs.qjournal.server">
  <!-- start interface org.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean -->
  <interface name="JournalNodeMXBean"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <method name="getJournalsStatus" return="java.lang.String"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Get status information (e.g., whether formatted) of JournalNode's journals.
 
 @return A string presenting status for each journal]]>
      </doc>
    </method>
    <doc>
    <![CDATA[This is the JMX management interface for JournalNode information]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean -->
</package>
<package name="org.apache.hadoop.hdfs.security.token.block">
</package>
<package name="org.apache.hadoop.hdfs.security.token.delegation">
</package>
<package name="org.apache.hadoop.hdfs.server.balancer">
</package>
<package name="org.apache.hadoop.hdfs.server.blockmanagement">
</package>
<package name="org.apache.hadoop.hdfs.server.common">
</package>
<package name="org.apache.hadoop.hdfs.server.datanode">
</package>
<package name="org.apache.hadoop.hdfs.server.datanode.fsdataset">
</package>
<package name="org.apache.hadoop.hdfs.server.datanode.fsdataset.impl">
</package>
<package name="org.apache.hadoop.hdfs.server.datanode.metrics">
</package>
<package name="org.apache.hadoop.hdfs.server.datanode.web">
</package>
<package name="org.apache.hadoop.hdfs.server.datanode.web.webhdfs">
</package>
<package name="org.apache.hadoop.hdfs.server.mover">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode">
  <!-- start interface org.apache.hadoop.hdfs.server.namenode.AuditLogger -->
  <interface name="AuditLogger"    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <method name="initialize"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="conf" type="org.apache.hadoop.conf.Configuration"/>
      <doc>
      <![CDATA[Called during initialization of the logger.

 @param conf The configuration object.]]>
      </doc>
    </method>
    <method name="logAuditEvent"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="succeeded" type="boolean"/>
      <param name="userName" type="java.lang.String"/>
      <param name="addr" type="java.net.InetAddress"/>
      <param name="cmd" type="java.lang.String"/>
      <param name="src" type="java.lang.String"/>
      <param name="dst" type="java.lang.String"/>
      <param name="stat" type="org.apache.hadoop.fs.FileStatus"/>
      <doc>
      <![CDATA[Called to log an audit event.
 <p>
 This method must return as quickly as possible, since it's called
 in a critical section of the NameNode's operation.

 @param succeeded Whether authorization succeeded.
 @param userName Name of the user executing the request.
 @param addr Remote address of the request.
 @param cmd The requested command.
 @param src Path of affected source file.
 @param dst Path of affected destination file (if any).
 @param stat File information for operations that change the file's
             metadata (permissions, owner, times, etc).]]>
      </doc>
    </method>
    <doc>
    <![CDATA[Interface defining an audit logger.]]>
    </doc>
  </interface>
  <!-- end interface org.apache.hadoop.hdfs.server.namenode.AuditLogger -->
  <!-- start class org.apache.hadoop.hdfs.server.namenode.HdfsAuditLogger -->
  <class name="HdfsAuditLogger" extends="java.lang.Object"
    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <implements name="org.apache.hadoop.hdfs.server.namenode.AuditLogger"/>
    <constructor name="HdfsAuditLogger"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="logAuditEvent"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="succeeded" type="boolean"/>
      <param name="userName" type="java.lang.String"/>
      <param name="addr" type="java.net.InetAddress"/>
      <param name="cmd" type="java.lang.String"/>
      <param name="src" type="java.lang.String"/>
      <param name="dst" type="java.lang.String"/>
      <param name="status" type="org.apache.hadoop.fs.FileStatus"/>
    </method>
    <method name="logAuditEvent"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="succeeded" type="boolean"/>
      <param name="userName" type="java.lang.String"/>
      <param name="addr" type="java.net.InetAddress"/>
      <param name="cmd" type="java.lang.String"/>
      <param name="src" type="java.lang.String"/>
      <param name="dst" type="java.lang.String"/>
      <param name="stat" type="org.apache.hadoop.fs.FileStatus"/>
      <param name="callerContext" type="org.apache.hadoop.ipc.CallerContext"/>
      <param name="ugi" type="org.apache.hadoop.security.UserGroupInformation"/>
      <param name="dtSecretManager" type="org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager"/>
      <doc>
      <![CDATA[Same as
 {@link #logAuditEvent(boolean, String, InetAddress, String, String, String,
 FileStatus)} with additional parameters related to logging delegation token
 tracking IDs.
 
 @param succeeded Whether authorization succeeded.
 @param userName Name of the user executing the request.
 @param addr Remote address of the request.
 @param cmd The requested command.
 @param src Path of affected source file.
 @param dst Path of affected destination file (if any).
 @param stat File information for operations that change the file's metadata
          (permissions, owner, times, etc).
 @param callerContext Context information of the caller
 @param ugi UserGroupInformation of the current user, or null if not logging
          token tracking information
 @param dtSecretManager The token secret manager, or null if not logging
          token tracking information]]>
      </doc>
    </method>
    <method name="logAuditEvent"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="succeeded" type="boolean"/>
      <param name="userName" type="java.lang.String"/>
      <param name="addr" type="java.net.InetAddress"/>
      <param name="cmd" type="java.lang.String"/>
      <param name="src" type="java.lang.String"/>
      <param name="dst" type="java.lang.String"/>
      <param name="stat" type="org.apache.hadoop.fs.FileStatus"/>
      <param name="ugi" type="org.apache.hadoop.security.UserGroupInformation"/>
      <param name="dtSecretManager" type="org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager"/>
      <doc>
      <![CDATA[Same as
 {@link #logAuditEvent(boolean, String, InetAddress, String, String,
 String, FileStatus, CallerContext, UserGroupInformation,
 DelegationTokenSecretManager)} without {@link CallerContext} information.]]>
      </doc>
    </method>
    <doc>
    <![CDATA[Extension of {@link AuditLogger}.]]>
    </doc>
  </class>
  <!-- end class org.apache.hadoop.hdfs.server.namenode.HdfsAuditLogger -->
  <!-- start class org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider -->
  <class name="INodeAttributeProvider" extends="java.lang.Object"
    abstract="true"
    static="false" final="false" visibility="public"
    deprecated="not deprecated">
    <constructor name="INodeAttributeProvider"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
    </constructor>
    <method name="start"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Initialize the provider. This method is called at NameNode startup
 time.]]>
      </doc>
    </method>
    <method name="stop"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <doc>
      <![CDATA[Shutdown the provider. This method is called at NameNode shutdown time.]]>
      </doc>
    </method>
    <method name="getAttributes" return="org.apache.hadoop.hdfs.server.namenode.INodeAttributes"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="fullPath" type="java.lang.String"/>
      <param name="inode" type="org.apache.hadoop.hdfs.server.namenode.INodeAttributes"/>
    </method>
    <method name="getAttributes" return="org.apache.hadoop.hdfs.server.namenode.INodeAttributes"
      abstract="true" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="pathElements" type="java.lang.String[]"/>
      <param name="inode" type="org.apache.hadoop.hdfs.server.namenode.INodeAttributes"/>
    </method>
    <method name="getAttributes" return="org.apache.hadoop.hdfs.server.namenode.INodeAttributes"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="components" type="byte[][]"/>
      <param name="inode" type="org.apache.hadoop.hdfs.server.namenode.INodeAttributes"/>
    </method>
    <method name="getExternalAccessControlEnforcer" return="org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider.AccessControlEnforcer"
      abstract="false" native="false" synchronized="false"
      static="false" final="false" visibility="public"
      deprecated="not deprecated">
      <param name="defaultEnforcer" type="org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider.AccessControlEnforcer"/>
      <doc>
      <![CDATA[Can be over-ridden by implementations to provide a custom Access Control
 Enforcer that can provide an alternate implementation of the
 default permission checking logic.
 @param defaultEnforcer The Default AccessControlEnforcer
 @return The AccessControlEnforcer to use]]>
      </doc>
    </method>
  </class>
  <!-- end class org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider -->
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.ha">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.metrics">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.snapshot">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.top">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.top.metrics">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.top.window">
</package>
<package name="org.apache.hadoop.hdfs.server.namenode.web.resources">
</package>
<package name="org.apache.hadoop.hdfs.server.protocol">
</package>
<package name="org.apache.hadoop.hdfs.tools">
</package>
<package name="org.apache.hadoop.hdfs.tools.offlineEditsViewer">
</package>
<package name="org.apache.hadoop.hdfs.tools.offlineImageViewer">
</package>
<package name="org.apache.hadoop.hdfs.tools.snapshot">
</package>
<package name="org.apache.hadoop.hdfs.util">
</package>
<package name="org.apache.hadoop.hdfs.web">
</package>
<package name="org.apache.hadoop.hdfs.web.resources">
</package>

</api>
