<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2024-10-22
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop Amazon Web Services support &#x2013; Working with Third-party S3 Stores</title>
    <style type="text/css" media="all">
      @import url("../../css/maven-base.css");
      @import url("../../css/maven-theme.css");
      @import url("../../css/site.css");
    </style>
    <link rel="stylesheet" href="../../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20241022" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                     <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://gitbox.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
              
                                   &nbsp;| Last Published: 2024-10-22
              &nbsp;| Version: 3.5.0-SNAPSHOT
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility Specification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DownstreamDev.html">Downstream Developer's Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html">Admin Compatibility Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FairCallQueue.html">Fair Call Queue</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/registry/index.html">Registry</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/AsyncProfilerServlet.html">Async Profiler</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html">Observer NameNode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFsOverloadScheme.html">ViewFsOverloadScheme</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">Disk Balancer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html">Router Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html">Provided Storage</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceModel.html">Resource Model</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeAttributes.html">Node Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/RuncContainers.html">Running Applications in runC Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/UsingGpus.html">Using GPU</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/UsingFPGA.html">Using FPGA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/PlacementConstraints.html">Placement Constraints</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnUI2.html">YARN UI2</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>YARN Service</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/Overview.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html">QuickStart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/Concepts.html">Concepts</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/YarnServiceAPI.html">Yarn Service API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/ServiceDiscovery.html">Service Discovery</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/SystemServices.html">System Services</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-aliyun/tools/hadoop-aliyun/index.html">Aliyun OSS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-cos/cloud-storage/index.html">Tencent COS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-huaweicloud/cloud-storage/index.html">Huaweicloud OBS</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-federation-balance/HDFSFederationBalance.html">HDFS Federation Balance</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-dynamometer/Dynamometer.html">Dynamometer</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/release/">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml">hdfs-rbf-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/kms-default.html">kms-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/httpfs-default.html">httpfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../../images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<h1>Working with Third-party S3 Stores</h1>
<p>The S3A connector works well with third-party S3 stores if the following requirements are met:</p>
<ul>

<li>It correctly implements the core S3 REST API, including support for uploads and the V2 listing API.</li>
<li>The store supports the AWS V4 signing API <i>or</i> a custom signer is switched to. This release does not support the legacy v2 signing API.</li>
<li>Errors are reported with the same HTTPS status codes as the S3 store. Error messages do not need to be consistent.</li>
<li>The store is consistent.</li>
</ul>
<p>There are also specific deployment requirements: * The clock on the store and the client are close enough that signing works. * The client is correctly configured to connect to the store <i>and not use unavailable features</i> * If HTTPS authentication is used, the client/JVM TLS configurations allows it to authenticate the endpoint.</p>
<p>The features which may be unavailable include:</p>
<ul>

<li>Checksum-based server-side change detection during copy/read (<code>fs.s3a.change.detection.mode=server</code>)</li>
<li>Object versioning and version-based change detection (<code>fs.s3a.change.detection.source=versionid</code> and <code>fs.s3a.versioned.store=true</code>)</li>
<li>Bulk delete (<code>fs.s3a.multiobjectdelete.enable=true</code>)</li>
<li>Encryption. (<code>fs.s3a.encryption.algorithm</code>)</li>
<li>Storage class set in <code>fs.s3a.create.storage.class</code></li>
<li>Content encodings as set with <code>fs.s3a.object.content.encoding</code>.</li>
<li>Optional Bucket Probes at startup (<code>fs.s3a.bucket.probe = 0</code>). This is now the default -do not change it.</li>
<li>List API to use (<code>fs.s3a.list.version = 1</code>)</li>
<li>Bucket lifecycle rules to clean up pending uploads.</li>
</ul><section><section>
<h3><a name="Disabling_Change_Detection"></a>Disabling Change Detection</h3>
<p>The (default) etag-based change detection logic expects stores to provide an Etag header in HEAD/GET requests, and to support it as a precondition in subsequent GET and COPY calls. If a store does not do this, disable the checks.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.change.detection.mode&lt;/name&gt;
  &lt;value&gt;none&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
</section></section><section>
<h2><a name="Connecting_to_a_third_party_object_store_over_HTTPS"></a>Connecting to a third party object store over HTTPS</h2>
<p>The core setting for a third party store is to change the endpoint in <code>fs.s3a.endpoint</code>.</p>
<p>This can be a URL or a hostname/hostname prefix For third-party stores without virtual hostname support, providing the URL is straightforward; path style access must also be enabled in <code>fs.s3a.path.style.access</code>.</p>
<p>The v4 signing algorithm requires a region to be set in <code>fs.s3a.endpoint.region</code>. A non-empty value is generally sufficient, though some deployments may require a specific value.</p>
<p>Finally, assuming the credential source is the normal access/secret key then these must be set, either in XML or (preferred) in a JCEKS file.</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
    &lt;value&gt;https://storeendpoint.example.com&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.endpoint.region&lt;/name&gt;
    &lt;value&gt;anything&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.access.key&lt;/name&gt;
    &lt;value&gt;13324445&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
    &lt;value&gt;4C6B906D-233E-4E56-BCEA-304CC73A14F8&lt;/value&gt;
  &lt;/property&gt;

</pre></div></div>

<p>If per-bucket settings are used here, then third-party stores and credentials may be used alongside an AWS store.</p></section><section>
<h2><a name="Other_issues"></a>Other issues</h2><section>
<h3><a name="Coping_without_bucket_lifecycle_rules"></a>Coping without bucket lifecycle rules</h3>
<p>Not all third-party stores support bucket lifecycle rules to clean up buckets of incomplete uploads.</p>
<p>This can be addressed in two ways * Command line: <code>hadoop s3guard uploads -abort -force \&lt;path&gt;</code>. * With <code>fs.s3a.multipart.purge</code> and a purge age set in <code>fs.s3a.multipart.purge.age</code> * In rename/delete <code>fs.s3a.directory.operations.purge.uploads = true</code>.</p><section>
<h4><a name="S3Guard_uploads_command"></a>S3Guard uploads command</h4>
<p>This can be executed on a schedule, or manually</p>

<div class="source">
<div class="source">
<pre>hadoop s3guard uploads -abort -force s3a://bucket/
</pre></div></div>

<p>Consult the <a href="s3guard.html">S3Guard documentation</a> for the full set of parameters.</p></section><section>
<h4><a name="In_startup:_fs.s3a.multipart.purge"></a>In startup: <code>fs.s3a.multipart.purge</code></h4>
<p>This lists all uploads in a bucket when a filesystem is created and deletes all of those above a certain age.</p>
<p>This can hurt performance on a large bucket, as the purge scans the entire tree, and is executed whenever a filesystem is created -which can happen many times during hive, spark, distcp jobs.</p>
<p>For this reason, this option may be deleted in future, however it has long been available in the S3A client and so guaranteed to work across versions.</p></section><section>
<h4><a name="During_rename_and_delete:_fs.s3a.directory.operations.purge.uploads"></a>During rename and delete: <code>fs.s3a.directory.operations.purge.uploads</code></h4>
<p>When <code>fs.s3a.directory.operations.purge.uploads</code> is set, when a directory is renamed or deleted, then in parallel with the delete an attempt is made to list all pending uploads. If there are any, they are aborted (sequentially).</p>
<ul>

<li>This is disabled by default: it adds overhead and extra cost.</li>
<li>Because it only applies to the directories being processed, directories which are not renamed or deleted will retain all incomplete uploads.</li>
<li>There is no age checking: all uploads will be aborted.</li>
<li>If any other process is writing to the same directory tree, their operations will be cancelled.</li>
</ul>
<h1>Troubleshooting</h1>
<p>The most common problem when talking to third-party stores are</p>
<ol style="list-style-type: decimal">

<li>The S3A client is still configured to talk to the AWS S3 endpoint. This leads to authentication failures and/or reports that the bucket is unknown.</li>
<li>Path access has not been enabled, the client is generating a host name for the target bucket and it does not exist.</li>
<li>Invalid authentication credentials.</li>
<li>JVM HTTPS settings include the certificates needed to negotiate a TLS connection with the store.</li>
</ol></section></section></section><section>
<h2><a name="How_to_improve_troubleshooting"></a>How to improve troubleshooting</h2><section>
<h3><a name="log_more_network_info"></a>log more network info</h3>
<p>There are some very low level logs.</p>

<div class="source">
<div class="source">
<pre># Log all HTTP requests made; includes S3 interaction. This may
# include sensitive information such as account IDs in HTTP headers.
log4j.logger.software.amazon.awssdk.request=DEBUG

# Turn on low level HTTP protocol debugging
log4j.logger.org.apache.http.wire=DEBUG

# async client
log4j.logger.io.netty.handler.logging=DEBUG
log4j.logger.io.netty.handler.codec.http2.Http2FrameLogger=DEBUG
</pre></div></div>
</section><section>
<h3><a name="Cut_back_on_retries.2C_shorten_timeouts"></a>Cut back on retries, shorten timeouts</h3>
<p>By default, there&#x2019;s a lot of retries going on in the AWS connector (which even retries on DNS failures) and in the S3A code which invokes it.</p>
<p>Normally this helps prevent long-lived jobs from failing due to a transient network problem, however it means that when trying to debug connectivity problems, the commands can hang for a long time as they keep trying to reconnect to ports which are never going to be available.</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;fs.iostatistics.logging.level&lt;/name&gt;
    &lt;value&gt;info&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.attempts.maximum&lt;/name&gt;
    &lt;value&gt;0&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
   &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.retry.limit&lt;/name&gt;
   &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.connection.timeout&lt;/name&gt;
    &lt;value&gt;500&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.connection.establish.timeout&lt;/name&gt;
    &lt;value&gt;500&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.retry.http.5xx.errors&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div>

<p>Setting the option <code>fs.s3a.retry.http.5xx.errors</code> to <code>false</code> stops the S3A client from treating 500 and other HTTP 5xx status codes other than 501 and 503 as errors to retry on. With AWS S3 they are eventually recovered from. On a third-party store they may be cause by other problems, such as:</p>
<ul>

<li>General service misconfiguration</li>
<li>Running out of disk storage</li>
<li>Storage Permissions</li>
</ul>
<p>Disabling the S3A client&#x2019;s retrying of these errors ensures that failures happen faster; the AWS SDK itself still makes a limited attempt to retry.</p></section></section><section>
<h2><a name="Cloudstore.E2.80.99s_Storediag"></a>Cloudstore&#x2019;s Storediag</h2>
<p>There&#x2019;s an external utility, <a class="externalLink" href="https://github.com/steveloughran/cloudstore">cloudstore</a> whose <a class="externalLink" href="https://github.com/steveloughran/cloudstore#command-storediag">storediag</a> exists to debug the connection settings to hadoop cloud storage.</p>

<div class="source">
<div class="source">
<pre>hadoop jar cloudstore-1.0.jar storediag s3a://nonexistent-bucket-example/
</pre></div></div>

<p>The main reason it&#x2019;s not an ASF release is that it allows for a rapid release cycle, sometimes hours; if anyone doesn&#x2019;t trust third-party code then they can download and build it themselves.</p>
<h1>Problems</h1></section><section>
<h2><a name="S3A_client_still_pointing_at_AWS_endpoint"></a>S3A client still pointing at AWS endpoint</h2>
<p>This is the most common initial problem, as it happens by default.</p>
<p>To fix, set <code>fs.s3a.endpoint</code> to the URL of the internal store.</p><section>
<h3><a name="org.apache.hadoop.fs.s3a.UnknownStoreException:s3a:.2F.2Fnonexistent-bucket-example.2F.E2.80.99:__Bucket_does_not_exist.60"></a><code>org.apache.hadoop.fs.s3a.UnknownStoreException:</code><a class="externalLink" href="s3a://nonexistent-bucket-example/">s3a://nonexistent-bucket-example/</a>&#x2019;:  Bucket does not exist`</h3>
<p>Either the bucket doesn&#x2019;t exist, or the bucket does exist but the endpoint is still set to an AWS endpoint.</p>

<div class="source">
<div class="source">
<pre>stat: `s3a://nonexistent-bucket-example/':  Bucket does not exist
</pre></div></div>

<p>The hadoop filesystem commands don&#x2019;t log stack traces on failure -adding this adds too much risk of breaking scripts, and the output is very uninformative</p>

<div class="source">
<div class="source">
<pre>stat: nonexistent-bucket-example: getS3Region on nonexistent-bucket-example:
software.amazon.awssdk.services.s3.model.S3Exception: null
(Service: S3, Status Code: 403, Request ID: X26NWV0RJ1697SXF, Extended Request ID: bqq0rRm5Bdwt1oHSfmWaDXTfSOXoYvNhQxkhjjNAOpxhRaDvWArKCFAdL2hDIzgec6nJk1BVpJE=):null
</pre></div></div>

<p>It is possible to turn on debugging</p>

<div class="source">
<div class="source">
<pre>log4j.logger.org.apache.hadoop.fs.shell=DEBUG
</pre></div></div>

<p>After which useful stack traces are logged.</p>

<div class="source">
<div class="source">
<pre>org.apache.hadoop.fs.s3a.UnknownStoreException: `s3a://nonexistent-bucket-example/':  Bucket does not exist
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$null$3(S3AFileSystem.java:1075)
    at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)
    at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)
    at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)
    at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:347)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getS3Region$4(S3AFileSystem.java:1039)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:543)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:524)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:445)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2631)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getS3Region(S3AFileSystem.java:1038)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:982)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:622)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3452)
</pre></div></div>
</section><section>
<h3><a name="S3Exception:_null_.28Service:_S3.2C_Status_Code:_403..._or_AccessDeniedException"></a><code>S3Exception: null (Service: S3, Status Code: 403...</code> or <code>AccessDeniedException</code></h3>
<ul>

<li>Endpoint is default</li>
<li>Credentials were not issued by AWS .</li>
<li><code>fs.s3a.endpoint.region</code> unset.</li>
</ul>
<p>If the client doesn&#x2019;t have any AWS credentials (from hadoop settings, environment variables or elsewhere) then the binding will fail even before the existence of the bucket can be probed for.</p>

<div class="source">
<div class="source">
<pre>hadoop fs -stat s3a://nonexistent-bucket-example
</pre></div></div>

<div class="source">
<div class="source">
<pre>stat: nonexistent-bucket-example: getS3Region on nonexistent-bucket-example:
software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 403,
 Request ID: X26NWV0RJ1697SXF, Extended Request ID: bqq0rRm5Bdwt1oHSfmWaDXTfSOXoYvNhQxkhjjNAOpxhRaDvWArKCFAdL2hDIzgec6nJk1BVpJE=):null
</pre></div></div>

<p>Or with a more detailed stack trace:</p>

<div class="source">
<div class="source">
<pre>java.nio.file.AccessDeniedException: nonexistent-bucket-example: getS3Region on nonexistent-bucket-example: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 403, Request ID: X26NWV0RJ1697SXF, Extended Request ID: bqq0rRm5Bdwt1oHSfmWaDXTfSOXoYvNhQxkhjjNAOpxhRaDvWArKCFAdL2hDIzgec6nJk1BVpJE=):null
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:235)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)
        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:347)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getS3Region$4(S3AFileSystem.java:1039)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:543)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:524)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:445)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2631)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.getS3Region(S3AFileSystem.java:1038)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:982)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:622)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3452)
</pre></div></div>
</section></section><section>
<h2><a name="Received_an_UnknownHostException_when_attempting_to_interact_with_a_service"></a><code>Received an UnknownHostException when attempting to interact with a service</code></h2><section>
<h3><a name="Hypothesis_1:_Region_set.2C_but_not_endpoint"></a>Hypothesis 1: Region set, but not endpoint</h3>
<p>The bucket <code>fs.s3a.endpoint.region</code> region setting is valid internally, but as the endpoint is still AWS, this region is not recognised. The S3A client&#x2019;s creation of an endpoint URL generates an unknown host.</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.endpoint.region&lt;/name&gt;
    &lt;value&gt;internal&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div>

<div class="source">
<div class="source">
<pre>ls: software.amazon.awssdk.core.exception.SdkClientException:
    Received an UnknownHostException when attempting to interact with a service.
    See cause for the exact endpoint that is failing to resolve.
    If this is happening on an endpoint that previously worked, there may be
    a network connectivity issue or your DNS cache could be storing endpoints for too long.:
    nonexistent-bucket-example.s3.internal.amazonaws.com: nodename nor servname provided, or not known


</pre></div></div>
</section><section>
<h3><a name="Hypothesis_2:_region_set.2C_endpoint_set.2C_but_fs.s3a.path.style.access_is_still_set_to_false"></a>Hypothesis 2: region set, endpoint set, but <code>fs.s3a.path.style.access</code> is still set to <code>false</code></h3>
<ul>

<li>The bucket <code>fs.s3a.endpoint.region</code> region setting is valid internally,</li>
<li>and <code>fs.s3a.endpoint</code> is set to a hostname (not a URL).</li>
<li><code>fs.s3a.path.style.access</code> set to <code>false</code></li>
</ul>

<div class="source">
<div class="source">
<pre>ls: software.amazon.awssdk.core.exception.SdkClientException:
    Received an UnknownHostException when attempting to interact with a service.
    See cause for the exact endpoint that is failing to resolve.
    If this is happening on an endpoint that previously worked, there may be
    a network connectivity issue or your DNS cache could be storing endpoints for too long.:
    nonexistent-bucket-example.localhost: nodename nor servname provided, or not known
</pre></div></div>

<p>Fix: path style access</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.nonexistent-bucket-example.path.style.access&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div>

<h1>Connecting to Google Cloud Storage through the S3A connector</h1>
<p>It <i>is</i> possible to connect to google cloud storage through the S3A connector. However, Google provide their own <a class="externalLink" href="https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage">Cloud Storage connector</a>. That is a well maintained Hadoop filesystem client which uses their XML API, And except for some very unusual cases, that is the connector to use.</p>
<p>When interacting with a GCS container through the S3A connector may make sense * The installation doesn&#x2019;t have the gcs-connector JAR. * The different credential mechanism may be convenient. * There&#x2019;s a desired to use S3A Delegation Tokens to pass secrets with a job. * There&#x2019;s a desire to use an external S3A extension (delegation tokens etc.)</p>
<p>The S3A connector binding works through the Google Cloud <a class="externalLink" href="https://cloud.google.com/distributed-cloud/hosted/docs/ga/gdch/apis/storage-s3-rest-api">S3 Storage API</a>, which is a subset of the AWS API.</p>
<p>To get a compatible access and secret key, follow the instructions of <a class="externalLink" href="https://cloud.google.com/storage/docs/aws-simple-migration#defaultproj">Simple migration from Amazon S3 to Cloud Storage</a>.</p>
<p>Here are the per-bucket setings for an example bucket &#x201c;gcs-container&#x201d; in Google Cloud Storage. Note the multiobject delete option must be disabled; this makes renaming and deleting significantly slower.</p>

<div class="source">
<div class="source">
<pre>&lt;configuration&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.access.key&lt;/name&gt;
    &lt;value&gt;GOOG1EZ....&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.secret.key&lt;/name&gt;
    &lt;value&gt;SECRETS&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.endpoint&lt;/name&gt;
    &lt;value&gt;https://storage.googleapis.com&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.bucket.probe&lt;/name&gt;
    &lt;value&gt;0&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.list.version&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.multiobjectdelete.enable&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.path.style.access&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.bucket.gcs-container.endpoint.region&lt;/name&gt;
    &lt;value&gt;dummy&lt;/value&gt;
  &lt;/property&gt;

&lt;/configuration&gt;
</pre></div></div>

<p>This is a very rarely used configuration -however, it can be done, possibly as a way to interact with Google Cloud Storage in a deployment which lacks the GCS connector.</p>
<p>It is also a way to regression test foundational S3A third-party store compatibility if you lack access to to any alternative.</p>

<div class="source">
<div class="source">
<pre>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;test.fs.s3a.encryption.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;fs.s3a.scale.test.csvfile&lt;/name&gt;
    &lt;value&gt;&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;test.fs.s3a.sts.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;test.fs.s3a.content.encoding.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</pre></div></div>

<p><i>Note</i> If anyone is set up to test this reguarly, please let the hadoop developer team know if regressions do surface, as it is not a common test configuration. []</p></section></section>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2008-2024
              Apache Software Foundation
            
                          - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
