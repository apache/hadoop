<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2024-10-22
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 3.5.0-SNAPSHOT &#x2013; Introduction</title>
    <style type="text/css" media="all">
      @import url("../css/maven-base.css");
      @import url("../css/maven-theme.css");
      @import url("../css/site.css");
    </style>
    <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20241022" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                     <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://gitbox.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                   &nbsp;| Last Published: 2024-10-22
              &nbsp;| Version: 3.5.0-SNAPSHOT
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility Specification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DownstreamDev.html">Downstream Developer's Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html">Admin Compatibility Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FairCallQueue.html">Fair Call Queue</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/registry/index.html">Registry</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/AsyncProfilerServlet.html">Async Profiler</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html">Observer NameNode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFsOverloadScheme.html">ViewFsOverloadScheme</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">Disk Balancer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html">Router Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html">Provided Storage</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceModel.html">Resource Model</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeAttributes.html">Node Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/RuncContainers.html">Running Applications in runC Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/UsingGpus.html">Using GPU</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/UsingFPGA.html">Using FPGA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/PlacementConstraints.html">Placement Constraints</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnUI2.html">YARN UI2</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>YARN Service</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/Overview.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html">QuickStart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/Concepts.html">Concepts</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/YarnServiceAPI.html">Yarn Service API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/ServiceDiscovery.html">Service Discovery</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/SystemServices.html">System Services</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-aliyun/tools/hadoop-aliyun/index.html">Aliyun OSS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-cos/cloud-storage/index.html">Tencent COS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-huaweicloud/cloud-storage/index.html">Huaweicloud OBS</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-federation-balance/HDFSFederationBalance.html">HDFS Federation Balance</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-dynamometer/Dynamometer.html">Dynamometer</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/release/">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml">hdfs-rbf-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/kms-default.html">kms-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/httpfs-default.html">httpfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<h1>Introduction</h1>
<ul>
<li><a href="#Naming">Naming</a></li>
<li><a href="#Implicit_assumptions_of_the_Hadoop_FileSystem_APIs">Implicit assumptions of the Hadoop FileSystem APIs</a>
<ul>
<li><a href="#Path_Names">Path Names</a></li>
<li><a href="#Security_Assumptions">Security Assumptions</a></li>
<li><a href="#Networking_Assumptions">Networking Assumptions</a></li></ul></li>
<li><a href="#Core_Expectations_of_a_Hadoop_Compatible_FileSystem">Core Expectations of a Hadoop Compatible FileSystem</a>
<ul>
<li><a href="#Atomicity">Atomicity</a></li>
<li><a href="#Consistency">Consistency</a></li>
<li><a href="#Concurrency">Concurrency</a></li>
<li><a href="#Operations_and_failures">Operations and failures</a></li>
<li><a href="#Undefined_capacity_limits">Undefined capacity limits</a></li>
<li><a href="#Undefined_timeouts">Undefined timeouts</a></li>
<li><a href="#Object_Stores_vs._Filesystems">Object Stores vs. Filesystems</a></li></ul></li></ul>

<p>This document defines the required behaviors of a Hadoop-compatible filesystem for implementors and maintainers of the Hadoop filesystem, and for users of the Hadoop FileSystem APIs</p>
<p>Most of the Hadoop operations are tested against HDFS in the Hadoop test suites, initially through <code>MiniDFSCluster</code>, before release by vendor-specific &#x2018;production&#x2019; tests, and implicitly by the Hadoop stack above it.</p>
<p>HDFS&#x2019;s actions have been modeled on POSIX filesystem behavior, using the actions and return codes of Unix filesystem actions as a reference. Even so, there are places where HDFS diverges from the expected behaviour of a POSIX filesystem.</p>
<p>The bundled S3A FileSystem clients make Amazon&#x2019;s S3 Object Store (&#x201c;blobstore&#x201d;) accessible through the FileSystem API. The Azure ABFS, WASB and ADL object storage FileSystems talks to Microsoft&#x2019;s Azure storage. All of these bind to object stores, which do have different behaviors, especially regarding consistency guarantees, and atomicity of operations.</p>
<p>The &#x201c;Local&#x201d; FileSystem provides access to the underlying filesystem of the platform. Its behavior is defined by the operating system and can behave differently from HDFS. Examples of local filesystem quirks include case-sensitivity, action when attempting to rename a file atop another file, and whether it is possible to <code>seek()</code> past the end of the file.</p>
<p>There are also filesystems implemented by third parties that assert compatibility with Apache Hadoop. There is no formal compatibility suite, and hence no way for anyone to declare compatibility except in the form of their own compatibility tests.</p>
<p>These documents <i>do not</i> attempt to provide a normative definition of compatibility. Passing the associated test suites <i>does not</i> guarantee correct behavior of applications.</p>
<p>What the test suites do define is the expected set of actions&#x2014;failing these tests will highlight potential issues.</p>
<p>By making each aspect of the contract tests configurable, it is possible to declare how a filesystem diverges from parts of the standard contract. This is information which can be conveyed to users of the filesystem.</p><section>
<h2><a name="Naming"></a>Naming</h2>
<p>This document follows RFC 2119 rules regarding the use of MUST, MUST NOT, MAY, and SHALL. MUST NOT is treated as normative.</p></section><section>
<h2><a name="Implicit_assumptions_of_the_Hadoop_FileSystem_APIs"></a>Implicit assumptions of the Hadoop FileSystem APIs</h2>
<p>The original <code>FileSystem</code> class and its usages are based on an implicit set of assumptions. Chiefly, that HDFS is the underlying FileSystem, and that it offers a subset of the behavior of a POSIX filesystem (or at least the implementation of the POSIX filesystem APIs and model provided by Linux filesystems).</p>
<p>Irrespective of the API, it&#x2019;s expected that all Hadoop-compatible filesystems present the model of a filesystem implemented in Unix:</p>
<ul>

<li>

<p>It&#x2019;s a hierarchical directory structure with files and directories.</p>
</li>
<li>

<p>Files contain zero or more bytes of data.</p>
</li>
<li>

<p>You cannot put files or directories under a file.</p>
</li>
<li>

<p>Directories contain zero or more files.</p>
</li>
<li>

<p>A directory entry has no data itself.</p>
</li>
<li>

<p>You can write arbitrary binary data to a file. When the file&#x2019;s contents are read, from anywhere inside or outside of the cluster, the data is returned.</p>
</li>
<li>

<p>You can store many gigabytes of data in a single file.</p>
</li>
<li>

<p>The root directory, <code>&quot;/&quot;</code>, always exists, and cannot be renamed.</p>
</li>
<li>

<p>The root directory, <code>&quot;/&quot;</code>, is always a directory, and cannot be overwritten by a file write operation.</p>
</li>
<li>

<p>Any attempt to recursively delete the root directory will delete its contents (barring lack of permissions), but will not delete the root path itself.</p>
</li>
<li>

<p>You cannot rename/move a directory under itself.</p>
</li>
<li>

<p>You cannot rename/move a directory atop any existing file other than the source file itself.</p>
</li>
<li>

<p>Directory listings return all the data files in the directory (i.e. there may be hidden checksum files, but all the data files are listed).</p>
</li>
<li>

<p>The attributes of a file in a directory listing (e.g. owner, length) match the actual attributes of a file, and are consistent with the view from an opened file reference.</p>
</li>
<li>

<p>Security: if the caller lacks the permissions for an operation, it will fail and raise an error.</p>
</li>
</ul><section>
<h3><a name="Path_Names"></a>Path Names</h3>
<ul>

<li>

<p>A Path is comprised of Path elements separated by <code>&quot;/&quot;</code>.</p>
</li>
<li>

<p>A path element is a unicode string of 1 or more characters.</p>
</li>
<li>

<p>Path element MUST NOT include the characters <code>&quot;:&quot;</code> or <code>&quot;/&quot;</code>.</p>
</li>
<li>

<p>Path element SHOULD NOT include characters of ASCII/UTF-8 value 0-31 .</p>
</li>
<li>

<p>Path element MUST NOT be <code>&quot;.&quot;</code>  or <code>&quot;..&quot;</code></p>
</li>
<li>

<p>Note also that the Azure blob store documents say that paths SHOULD NOT use a trailing <code>&quot;.&quot;</code> (as their .NET URI class strips it).</p>
</li>
<li>

<p>Paths are compared based on unicode code-points.</p>
</li>
<li>

<p>Case-insensitive and locale-specific comparisons MUST NOT not be used.</p>
</li>
</ul></section><section>
<h3><a name="Security_Assumptions"></a>Security Assumptions</h3>
<p>Except in the special section on security, this document assumes the client has full access to the FileSystem. Accordingly, the majority of items in the list do not add the qualification &#x201c;assuming the user has the rights to perform the operation with the supplied parameters and paths&#x201d;.</p>
<p>The failure modes when a user lacks security permissions are not specified.</p></section><section>
<h3><a name="Networking_Assumptions"></a>Networking Assumptions</h3>
<p>This document assumes that all network operations succeed. All statements can be assumed to be qualified as <i>&#x201c;assuming the operation does not fail due to a network availability problem&#x201d;</i></p>
<ul>

<li>

<p>The final state of a FileSystem after a network failure is undefined.</p>
</li>
<li>

<p>The immediate consistency state of a FileSystem after a network failure is undefined.</p>
</li>
<li>

<p>If a network failure can be reported to the client, the failure MUST be an instance of <code>IOException</code> or subclass thereof.</p>
</li>
<li>

<p>The exception details SHOULD include diagnostics suitable for an experienced Java developer <i>or</i> operations team to begin diagnostics. For example, source and destination hostnames and ports on a ConnectionRefused exception.</p>
</li>
<li>

<p>The exception details MAY include diagnostics suitable for inexperienced developers to begin diagnostics. For example Hadoop tries to include a reference to <a class="externalLink" href="http://wiki.apache.org/hadoop/ConnectionRefused">ConnectionRefused</a> when a TCP connection request is refused.</p>
</li>
</ul><!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
</section></section><section>
<h2><a name="Core_Expectations_of_a_Hadoop_Compatible_FileSystem"></a>Core Expectations of a Hadoop Compatible FileSystem</h2>
<p>Here are the core expectations of a Hadoop-compatible FileSystem. Some FileSystems do not meet all these expectations; as a result, some programs may not work as expected.</p><section>
<h3><a name="Atomicity"></a>Atomicity</h3>
<p>There are some operations that MUST be atomic. This is because they are often used to implement locking/exclusive access between processes in a cluster.</p>
<ol style="list-style-type: decimal">

<li>Creating a file. If the <code>overwrite</code> parameter is false, the check and creation MUST be atomic.</li>
<li>Deleting a file.</li>
<li>Renaming a file.</li>
<li>Renaming a directory.</li>
<li>Creating a single directory with <code>mkdir()</code>.</li>
</ol>
<ul>

<li>Recursive directory deletion MAY be atomic. Although HDFS offers atomic recursive directory deletion, none of the other Hadoop FileSystems offer such a guarantee (including local FileSystems).</li>
</ul>
<p>Most other operations come with no requirements or guarantees of atomicity.</p></section><section>
<h3><a name="Consistency"></a>Consistency</h3>
<p>The consistency model of a Hadoop FileSystem is <i>one-copy-update-semantics</i>; that of a traditional local POSIX filesystem. Note that even NFS relaxes some constraints about how fast changes propagate.</p>
<ul>

<li>

<p><i>Create.</i> Once the <code>close()</code> operation on an output stream writing a newly created file has completed, in-cluster operations querying the file metadata and contents MUST immediately see the file and its data.</p>
</li>
<li>

<p><i>Update.</i> Once the <code>close()</code>  operation on an output stream writing a newly created file has completed, in-cluster operations querying the file metadata and contents MUST immediately see the new data.</p>
</li>
<li>

<p><i>Delete.</i> once a <code>delete()</code> operation on a path other than &#x201c;/&#x201d; has completed successfully, it MUST NOT be visible or accessible. Specifically, <code>listStatus()</code>, <code>open()</code> ,<code>rename()</code> and <code>append()</code> operations MUST fail.</p>
</li>
<li>

<p><i>Delete then create.</i> When a file is deleted then a new file of the same name created, the new file MUST be immediately visible and its contents accessible via the FileSystem APIs.</p>
</li>
<li>

<p><i>Rename.</i> After a <code>rename()</code>  has completed, operations against the new path MUST succeed; attempts to access the data against the old path MUST fail.</p>
</li>
<li>

<p>The consistency semantics inside of the cluster MUST be the same as outside of the cluster. All clients querying a file that is not being actively manipulated MUST see the same metadata and data irrespective of their location.</p>
</li>
</ul></section><section>
<h3><a name="Concurrency"></a>Concurrency</h3>
<p>There are no guarantees of isolated access to data: if one client is interacting with a remote file and another client changes that file, the changes may or may not be visible.</p></section><section>
<h3><a name="Operations_and_failures"></a>Operations and failures</h3>
<ul>

<li>

<p>All operations MUST eventually complete, successfully or unsuccessfully.</p>
</li>
<li>

<p>The time to complete an operation is undefined and may depend on the implementation and on the state of the system.</p>
</li>
<li>

<p>Operations MAY throw a <code>RuntimeException</code> or subclass thereof.</p>
</li>
<li>

<p>Operations SHOULD raise all network, remote, and high-level problems as an <code>IOException</code> or subclass thereof, and SHOULD NOT raise a <code>RuntimeException</code> for such problems.</p>
</li>
<li>

<p>Operations SHOULD report failures by way of raised exceptions, rather than specific return codes of an operation.</p>
</li>
<li>

<p>In the text, when an exception class is named, such as <code>IOException</code>, the raised exception MAY be an instance or subclass of the named exception. It MUST NOT be a superclass.</p>
</li>
<li>

<p>If an operation is not implemented in a class, the implementation must throw an <code>UnsupportedOperationException</code>.</p>
</li>
<li>

<p>Implementations MAY retry failed operations until they succeed. If they do this, they SHOULD do so in such a way that the <i>happens-before</i> relationship between any sequence of operations meets the consistency and atomicity requirements stated. See <a class="externalLink" href="https://issues.apache.org/jira/browse/HDFS-4849">HDFS-4849</a> for an example of this: HDFS does not implement any retry feature that could be observable by other callers.</p>
</li>
</ul></section><section>
<h3><a name="Undefined_capacity_limits"></a>Undefined capacity limits</h3>
<p>Here are some limits to FileSystem capacity that have never been explicitly defined.</p>
<ol style="list-style-type: decimal">

<li>

<p>The maximum number of files in a directory.</p>
</li>
<li>

<p>Max number of directories in a directory</p>
</li>
<li>

<p>Maximum total number of entries (files and directories) in a filesystem.</p>
</li>
<li>

<p>The maximum length of a filename under a directory (HDFS: 8000).</p>
</li>
<li>

<p><code>MAX_PATH</code> - the total length of the entire directory tree referencing a file. Blobstores tend to stop at ~1024 characters.</p>
</li>
<li>

<p>The maximum depth of a path (HDFS: 1000 directories).</p>
</li>
<li>

<p>The maximum size of a single file.</p>
</li>
</ol></section><section>
<h3><a name="Undefined_timeouts"></a>Undefined timeouts</h3>
<p>Timeouts for operations are not defined at all, including:</p>
<ul>

<li>

<p>The maximum completion time of blocking FS operations. MAPREDUCE-972 documents how <code>distcp</code> broke on slow s3 renames.</p>
</li>
<li>

<p>The timeout for idle read streams before they are closed.</p>
</li>
<li>

<p>The timeout for idle write streams before they are closed.</p>
</li>
</ul>
<p>The blocking-operation timeout is in fact variable in HDFS, as sites and clients may tune the retry parameters so as to convert filesystem failures and failovers into pauses in operation. Instead there is a general assumption that FS operations are &#x201c;fast but not as fast as local FS operations&#x201d;, and that the latency of data reads and writes scale with the volume of data. This assumption by client applications reveals a more fundamental one: that the filesystem is &#x201c;close&#x201d; as far as network latency and bandwidth is concerned.</p>
<p>There are also some implicit assumptions about the overhead of some operations.</p>
<ol style="list-style-type: decimal">

<li>

<p><code>seek()</code> operations are fast and incur little or no network delays. [This does not hold on blob stores]</p>
</li>
<li>

<p>Directory list operations are fast for directories with few entries.</p>
</li>
<li>

<p>Directory list operations are fast for directories with few entries, but may incur a cost that is <code>O(entries)</code>. Hadoop 2 added iterative listing to handle the challenge of listing directories with millions of entries without buffering at the cost of consistency.</p>
</li>
<li>

<p>A <code>close()</code> of an <code>OutputStream</code> is fast, irrespective of whether or not the file operation has succeeded or not.</p>
</li>
<li>

<p>The time to delete a directory is independent of the size of the number of child entries</p>
</li>
</ol></section><section>
<h3><a name="Object_Stores_vs._Filesystems"></a>Object Stores vs. Filesystems</h3>
<p>This specification refers to <i>Object Stores</i> in places, often using the term <i>Blobstore</i>. Hadoop does provide FileSystem client classes for some of these even though they violate many of the requirements.</p>
<p>Consult the documentation for a specific store to determine its compatibility with specific applications and services.</p>
<p><i>What is an Object Store?</i></p>
<p>An object store is a data storage service, usually accessed over HTTP/HTTPS. A <code>PUT</code> request uploads an object/&#x201c;Blob&#x201d;; a <code>GET</code> request retrieves it; ranged <code>GET</code> operations permit portions of a blob to retrieved. To delete the object, the HTTP <code>DELETE</code> operation is invoked.</p>
<p>Objects are stored by name: a string, possibly with &#x201c;/&#x201d; symbols in them. There is no notion of a directory; arbitrary names can be assigned to objects &#x2014; within the limitations of the naming scheme imposed by the service&#x2019;s provider.</p>
<p>The object stores invariably provide an operation to retrieve objects with a given prefix; a <code>GET</code> operation on the root of the service with the appropriate query parameters.</p>
<p>Object stores usually prioritize availability &#x2014;there is no single point of failure equivalent to the HDFS NameNode(s). They also strive for simple non-POSIX APIs: the HTTP verbs are the operations allowed.</p>
<p>Hadoop FileSystem clients for object stores attempt to make the stores pretend that they are a FileSystem, a FileSystem with the same features and operations as HDFS. This is &#x2014;ultimately&#x2014;a pretence: they have different characteristics and occasionally the illusion fails.</p>
<ol style="list-style-type: decimal">

<li>

<p><b>Consistency</b>. Object may be <i>Eventually Consistent</i>: it can take time for changes to objects &#x2014;creation, deletion and updates&#x2014; to become visible to all callers. Indeed, there is no guarantee a change is immediately visible to the client which just made the change. As an example, an object <code>test/data1.csv</code> may be overwritten with a new set of data, but when a <code>GET test/data1.csv</code> call is made shortly after the update, the original data returned. Hadoop assumes that filesystems are consistent; that creation, updates and deletions are immediately visible, and that the results of listing a directory are current with respect to the files within that directory.</p>
</li>
<li>

<p><b>Atomicity</b>. Hadoop assumes that directory <code>rename()</code> operations are atomic, as are <code>delete()</code> operations. Object store FileSystem clients implement these as operations on the individual objects whose names match the directory prefix. As a result, the changes take place a file at a time, and are not atomic. If an operation fails part way through the process, then the state of the object store reflects the partially completed operation.  Note also that client code assumes that these operations are <code>O(1)</code> &#x2014;in an object store they are more likely to be <code>O(child-entries)</code>.</p>
</li>
<li>

<p><b>Durability</b>. Hadoop assumes that <code>OutputStream</code> implementations write data to their (persistent) storage on a <code>flush()</code> operation. Object store implementations save all their written data to a local file, a file that is then only <code>PUT</code> to the object store in the final <code>close()</code> operation. As a result, there is never any partial data from incomplete or failed operations. Furthermore, as the write process only starts in  <code>close()</code> operation, that operation may take a time proportional to the quantity of data to upload, and inversely proportional to the network bandwidth. It may also fail &#x2014;a failure that is better escalated than ignored.</p>
</li>
<li>

<p><b>Authorization</b>. Hadoop uses the <code>FileStatus</code> class to represent core metadata of files and directories, including the owner, group and permissions.  Object stores might not have a viable way to persist this metadata, so they might need to populate <code>FileStatus</code> with stub values.  Even if the object store persists this metadata, it still might not be feasible for the object store to enforce file authorization in the same way as a traditional file system.  If the object store cannot persist this metadata, then the recommended convention is:</p>
<ul>

<li>File owner is reported as the current user.</li>
<li>File group also is reported as the current user.</li>
<li>Directory permissions are reported as 777.</li>
<li>File permissions are reported as 666.</li>
<li>File system APIs that set ownership and permissions execute successfully without error, but they are no-ops.</li>
</ul>
</li>
</ol>
<p>Object stores with these characteristics, can not be used as a direct replacement for HDFS. In terms of this specification, their implementations of the specified operations do not match those required. They are considered supported by the Hadoop development community, but not to the same extent as HDFS.</p><section>
<h4><a name="Timestamps"></a>Timestamps</h4>
<p><code>FileStatus</code> entries have a modification time and an access time.</p>
<ol style="list-style-type: decimal">

<li>The exact behavior as to when these timestamps are set and whether or not they are valid varies between filesystems, and potentially between individual installations of a filesystem.</li>
<li>The granularity of the timestamps is again, specific to both a filesystem and potentially individual installations.</li>
</ol>
<p>The HDFS filesystem does not update the modification time while it is being written to.</p>
<p>Specifically</p>
<ul>

<li><code>FileSystem.create()</code> creation: a zero-byte file is listed; the modification time is set to the current time as seen on the NameNode.</li>
<li>Writes to a file via the output stream returned in the <code>create()</code> call: the modification time <i>does not change</i>.</li>
<li>When <code>OutputStream.close()</code> is called, all remaining data is written, the file closed and the NameNode updated with the final size of the file. The modification time is set to the time the file was closed.</li>
<li>Opening a file for appends via an <code>append()</code> operation does not change the modification time of the file until the <code>close()</code> call is made on the output stream.</li>
<li><code>FileSystem.setTimes()</code> can be used to explicitly set the time on a file.</li>
<li>When a file is renamed, its modification time is not changed, but the source and destination directories have their modification times updated.</li>
<li>The rarely used operations:  <code>FileSystem.concat()</code>, <code>createSnapshot()</code>, <code>createSymlink()</code> and <code>truncate()</code> all update the modification time.</li>
<li>The access time granularity is set in milliseconds <code>dfs.namenode.access.time.precision</code>; the default granularity is 1 hour. If the precision is set to zero, access times are not recorded.</li>
<li>If a modification or access time is not set, the value of that <code>FileStatus</code> field is 0.</li>
</ul>
<p>Other filesystems may have different behaviors. In particular,</p>
<ul>

<li>Access times may or may not be supported; even if the underlying FS may support access times, the option it is often disabled for performance reasons.</li>
<li>The granularity of the timestamps is an implementation-specific detail.</li>
</ul>
<p>Object stores have an even vaguer view of time, which can be summarized as &#x201c;it varies&#x201d;.</p>
<ul>

<li>The timestamp granularity is likely to be 1 second, that being the granularity of timestamps returned in HTTP HEAD and GET requests.</li>
<li>Access times are likely to be unset. That is, <code>FileStatus.getAccessTime() == 0</code>.</li>
<li>The modification timestamp for a newly created file MAY be that of the <code>create()</code> call, or the actual time which the PUT request was initiated. This may be in the  <code>FileSystem.create()</code> call, the final <code>OutputStream.close()</code> operation, some period in between.</li>
<li>The modification time may not be updated in the <code>close()</code> call.</li>
<li>The timestamp is likely to be in UTC or the TZ of the object store. If the client is in a different timezone, the timestamp of objects may be ahead or behind that of the client.</li>
<li>A file&#x2019;s modification time is often the same as its creation time.</li>
<li>The <code>FileSystem.setTimes()</code> operation to set file timestamps <i>may</i> be ignored.</li>
<li><code>FileSystem.chmod()</code> may update modification times (example: Azure <code>wasb://</code>).</li>
<li>If <code>FileSystem.append()</code> is supported, the changes and modification time are likely to only become visible after the output stream is closed.</li>
<li>Out-of-band operations to data in object stores (that is: direct requests to object stores which bypass the Hadoop FileSystem APIs), may result in different timestamps being stored and/or returned.</li>
<li>As the notion of a directory structure is often simulated, the timestamps of directories <i>may</i> be artificially generated &#x2014;perhaps using the current system time.</li>
<li>As <code>rename()</code> operations are often implemented as a COPY + DELETE, the timestamps of renamed objects may become that of the time the rename of an object was started, rather than the timestamp of the source object.</li>
<li>The exact timestamp behavior may vary between different object store installations, even with the same timestore client.</li>
</ul>
<p>Finally, note that the Apache Hadoop project cannot make any guarantees about whether the timestamp behavior of a remote object store will remain consistent over time: they are third-party services, usually accessed via third-party libraries.</p>
<p>The best strategy here is &#x201c;experiment with the exact endpoint you intend to work with&#x201d;. Furthermore, if you intend to use any caching/consistency layer, test with that feature enabled. Retest after updates to Hadoop releases, and endpoint object store updates.</p></section></section></section>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2008-2024
              Apache Software Foundation
            
                          - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
