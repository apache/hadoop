<?xml version="1.0" encoding="UTF-8"?>
<!--
/**
 * Copyright 2010 The Apache Software Foundation
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<book version="5.0" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:svg="http://www.w3.org/2000/svg"
      xmlns:m="http://www.w3.org/1998/Math/MathML"
      xmlns:html="http://www.w3.org/1999/xhtml"
      xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <title>The Apache <link xlink:href="http://www.hbase.org">HBase</link>
    Book</title>
      <copyright><year>2010</year><holder>Apache Software Foundation</holder></copyright>
      <abstract>
    <para>This is the official book of
    <link xlink:href="http://www.hbase.org">Apache HBase</link>,
    a distributed, versioned, column-oriented database built on top of
    Apache Hadoop <link xlink:href="http://hadoop.apache.org/">Common and HDFS</link>.
      </para>
      </abstract>

    <revhistory>
      <revision>
        <date />

        <revdescription>Adding first cuts at Configuration, Getting Started, Data Model</revdescription>
        <revnumber>
          <?eval ${project.version}?>
        </revnumber>
      </revision>
      <revision>
        <date>
        5 October 2010
        </date>
        <authorinitials>stack</authorinitials>
        <revdescription>Initial layout</revdescription>
        <revnumber>
          0.89.20100924
        </revnumber>
      </revision>
    </revhistory>
  </info>

  <preface xml:id="preface">
    <title>Preface</title>

    <para>This book aims to be the official guide for the <link
    xlink:href="http://hbase.apache.org/">HBase</link> version it ships with.
    This document describes HBase version <emphasis><?eval ${project.version}?></emphasis>.
    Herein you will find either the definitive documentation on an HBase topic
    as of its standing when the referenced HBase version shipped, or failing
    that, this book will point to the location in <link
    xlink:href="http://hbase.apache.org/docs/current/api/index.html">javadoc</link>,
    <link xlink:href="https://issues.apache.org/jira/browse/HBASE">JIRA</link>
    or <link xlink:href="http://wiki.apache.org/hadoop/Hbase">wiki</link>
    where the pertinent information can be found.</para>

    <para>This book is a work in progress. It is lacking in many areas but we
    hope to fill in the holes with time. Feel free to add to this book should
    you feel so inclined by adding a patch to an issue up in the HBase <link
    xlink:href="https://issues.apache.org/jira/browse/HBASE">JIRA</link>.</para>
  </preface>

  <chapter xml:id="getting_started">
    <title>Getting Started</title>

    <section xml:id="quickstart">
      <title>Quick Start</title>

      <para><itemizedlist>
          <para>Here is a quick guide to starting up a standalone HBase
          instance (an HBase instance that uses the local filesystem rather than
          Hadoop HDFS), creating a table and inserting rows into a table via the
          <link linkend="shell">HBase Shell</link>, and then cleaning up and shutting
          down your running instance. The below exercise should take no more than
          ten minutes (not including download time).
      </para>
          
          <listitem>
            <para>Download and unpack the latest stable release.</para>

            <para>Choose a download site from this list of <link
            xlink:href="http://www.apache.org/dyn/closer.cgi/hbase/">Apache
            Download Mirrors</link>. Click on it. This will take you to a
            mirror of <emphasis>HBase Releases</emphasis>. Click on
            the folder named <filename>stable</filename> and then download the
            file that ends in <filename>.tar.gz</filename> to your local filesystem;
            e.g. <filename>hbase-<?eval ${project.version}?>.tar.gz</filename>.</para>

            <para>Decompress and untar your download and then change into the
            unpacked directory.</para>

            <para><programlisting>$ tar xfz hbase-<?eval ${project.version}?>.tar.gz
$ cd hbase-<?eval ${project.version}?>
</programlisting></para>

<para>
   At this point, you are ready to start HBase. But before starting it,
   edit <filename>conf/hbase-site.xml</filename> and set the directory
   you want HBase to write to, <varname>hbase.rootdir</varname>.
   <programlisting>
<![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///DIRECTORY/hbase</value>
  </property>
</configuration>
]]>
</programlisting>
Replace <varname>DIRECTORY</varname> in the above with a path to a directory where you want
HBase to store its data.  By default, <varname>hbase.rootdir</varname> is set to <filename>/tmp/hbase-${user.name}</filename> 
which means you'll lose all your data whenever your server reboots.
</para>

            <para>Now start HBase:<programlisting>$ ./bin/start-hbase.sh
starting master, logging to logs/hbase-user-master-example.org.out</programlisting></para>

            <para>You now have a running standalone HBase instance. In standalone mode, HBase runs
            all daemons in the the one JVM; i.e. the master, regionserver, and zookeeper daemons.
            Also by default, HBase in standalone mode writes data to <filename>/tmp/hbase-${USERID}</filename>.
            HBase logs can be found in the <filename>logs</filename> subdirectory. Check them
            out especially if HBase had trouble starting.</para>

            <note>
            <title>Is <application>java</application> installed?</title>
            <para>The above presumes a 1.6 version of Oracle
            <application>java</application> is installed on your
            machine and available on your path; i.e. when you type
            <application>java</application>, you see output that describes the options
            the java program takes (HBase like Hadoop requires java 6).  If this is
            not the case, HBase will not start.
            Install java, edit <filename>conf/hbase-env.sh</filename>, uncommenting the
            <envar>JAVA_HOME</envar> line pointing it  to your java install.  Then,
            retry the steps above.</para>
            </note>
            
          </listitem>

          <listitem>
            <para>Connect to your running HBase via the 
          <link linkend="shell">HBase Shell</link>.</para>

            <para><programlisting>$ ./bin/hbase shell
HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.
Type "exit&lt;RETURN&gt;" to leave the HBase Shell
Version: 0.89.20100924, r1001068, Fri Sep 24 13:55:42 PDT 2010

hbase(main):001:0&gt; </programlisting></para>

            <para>Type <command>help</command> and then <command>&lt;RETURN&gt;</command>
            to see a listing of shell
            commands and options. Browse at least the paragraphs at the end of
            the help emission for the gist of how variables are entered in the
            HBase shell; in particular note how table names, rows, and
            columns, etc., must be quoted.</para>
          </listitem>

          <listitem>
            <para>Create a table named <filename>test</filename> with a single
            column family named <filename>cf.</filename>.  Verify its creation by
            listing all tables and then insert some
            values.</para>
            <para><programlisting>hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'table'
test
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds</programlisting></para>

            <para>Above we inserted 3 values, one at a time. The first insert is at
            <varname>row1</varname>, column <varname>cf:a</varname> -- columns
            have a column family prefix delimited by the colon character --
            with a value of <varname>value1</varname>.</para>
          </listitem>

          <listitem>
            <para>Verify the table content</para>

            <para>Run a scan of the table by doing the following</para>

            <para><programlisting>hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds</programlisting></para>

            <para>Get a single row as follows</para>

            <para><programlisting>hbase(main):008:0&gt; get 'test', 'row1'
COLUMN      CELL
cf:a        timestamp=1288380727188, value=value1
1 row(s) in 0.0400 seconds</programlisting></para>
          </listitem>

          <listitem>
            <para>Now, disable and drop your table. This will clean up all
            done above.</para>

            <para><programlisting>hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds </programlisting></para>
          </listitem>

          <listitem>
            <para>Exit the shell by typing exit.</para>

            <para><programlisting>hbase(main):014:0&gt; exit</programlisting></para>
          </listitem>

          <listitem>
            <para>Stop your hbase instance by running the stop script.</para>

            <para><programlisting>$ ./bin/stop-hbase.sh
stopping hbase...............</programlisting></para>
          </listitem>
        </itemizedlist>
      </para>
      <section><title>Where to go next
      </title>
      <para>The above described standalone setup is good for testing and experiments only.
      Move on to the next section, the <link linkend="notsoquick">Not-so-quick Start Guide</link>
      where we'll go into depth on the different HBase run modes, requirements and critical
      configurations needed setting up a distributed HBase deploy.
      </para>
      </section>
    </section>

    <section xml:id="notsoquick">
      <title>Not-so-quick Start Guide</title>
      <section xml:id="requirements"><title>Requirements</title>
      <para>HBase has the following requirements.  Please read the
      section below carefully and ensure that all requirements have been
      satisfied.  Failure to do so will cause you (and us) grief debugging
      strange errors and/or data loss.
      </para>

  <section xml:id="java"><title>java</title>
<para>
  Just like Hadoop, HBase requires java 6 from <link xlink:href="http://www.java.com/download/">Oracle</link>.
Usually you'll want to use the latest version available except the problematic u18  (u22 is the latest version as of this writing).</para>
</section>
  <section xml:id="hadoop"><title><link xlink:href="http://hadoop.apache.org">hadoop</link></title>
<para>This version of HBase will only run on <link xlink:href="http://hadoop.apache.org/common/releases.html">Hadoop 0.20.x</link>.
 HBase will lose data unless it is running on an HDFS that has a durable sync.
 Currently only the <link xlink:href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/">branch-0.20-append</link>
 branch has this attribute.  No official releases have been made from this branch as of this writing
 so you will have to build your own Hadoop from the tip of this branch
 (or install Cloudera's <link xlink:href="http://archive.cloudera.com/docs/">CDH3</link> (as of this writing, it is in beta); it has the
 0.20-append patches needed to add a durable sync).
 See <link xlink:href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/CHANGES.txt">CHANGES.txt</link>
 in branch-0.20.-append to see list of patches involved.</para>
  </section>
<section xml:id="ssh"> <title>ssh</title>
<para><command>ssh</command> must be installed and <command>sshd</command> must be running to use Hadoop's scripts to manage remote Hadoop daemons.
   You must be able to ssh to all nodes, including your local node, using passwordless login (Google "ssh passwordless login").
  </para>
</section>
  <section><title>DNS</title>
<para>Basic name resolving must be working correctly on your cluster.
</para>
</section>
  <section><title>NTP</title>
<para>
    The clocks on cluster members should be in basic alignments. Some skew is tolerable but
    wild skew could generate odd behaviors. Run <link xlink:href="http://en.wikipedia.org/wiki/Network_Time_Protocol">NTP</link>
    on your cluster, or an equivalent.
  </para>
</section>

      <section xml:id="ulimit">
      <title><varname>ulimit</varname></title>
      <para>HBase is a database, it uses a lot of files at the same time.
      The default ulimit -n of 1024 on *nix systems is insufficient.
      Any significant amount of loading will lead you to 
      <link xlink:href="http://wiki.apache.org/hadoop/Hbase/FAQ#A6">FAQ: Why do I see "java.io.IOException...(Too many open files)" in my logs?</link>.
      You will also notice errors like:
      <programlisting>2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
      </programlisting>
      Do yourself a favor and change the upper bound on the number of file descriptors.
      Set it to north of 10k.  See the above referenced FAQ for how.</para>
      <para>To be clear, upping the file descriptors for the user who is
      running the HBase process is an operating system configuration, not an
      HBase configuration.
      </para>
      </section>

      <section xml:id="dfs.datanode.max.xcievers">
      <title><varname>dfs.datanode.max.xcievers</varname></title>
      <para>
      Hadoop HDFS has an upper bound of files that it will serve at one same time,
      called <varname>xcievers</varname> (yes, this is misspelled). Again, before
      doing any loading, make sure you have configured Hadoop's <filename>conf/hdfs-site.xml</filename>
      setting the <varname>xceivers</varname> value to at least the following:
      <programlisting>
      &lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;2047&lt;/value&gt;
      &lt;/property&gt;
      </programlisting>
      </para>
      </section>
      </section>

      <section><title>HBase run modes: Standalone, Pseudo-distributed, and Distributed</title>
      <para>HBase has three different run modes: standalone, this is what is described above in
      <link linkend="quickstart">Quick Start,</link> pseudo-distributed mode where all
      daemons run on a single server, and distributed, where each of the daemons runs
      on different cluster node.</para>
      <section><title>Standalone HBase</title>
      <para>TODO</para>
      </section>
      <section><title>Pseudo-distributed</title>
      <para>TODO</para>
      </section>
      <section><title>Distributed</title>
      <para>TODO</para>
      </section>
      </section>
      <section><title>Client configuration and dependencies connecting to an HBase cluster</title>
      <para>TODO</para>
      </section>

    <section><title>Example Configurations</title>
    <para>In this section we provide a few sample configurations.</para>
    <section><title>Basic Distributed HBase Install</title>
    <para>Here is example basic configuration of a ten node cluster running in
    distributed mode.  The nodes
are named <varname>example0</varname>, <varname>example1</varname>, etc., through
node <varname>example9</varname>  in this example.  The HBase Master and the HDFS namenode 
are running on the node <varname>example0</varname>.  RegionServers run on nodes
<varname>example1</varname>-<varname>example9</varname>.
A 3-node zookeeper ensemble runs on <varname>example1</varname>, <varname>example2</varname>, and <varname>example3</varname>.
Below we show what the main configuration files
-- <filename>hbase-site.xml</filename>, <filename>regionservers</filename>, and
<filename>hbase-env.sh</filename> -- found in the <filename>conf</filename> directory
might look like.
</para>
    <section><title><filename>hbase-site.xml</filename></title>
    <programlisting>
<![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by region servers.
    </description>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/export/stack/zookeeper</value>
    <description>Property from ZooKeeper's config zoo.cfg.
    The directory where the snapshot is stored.
    </description>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://example1:9000/hbase</value>
    <description>The directory shared by region servers.
    </description>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
    <description>The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    </description>
  </property>
</configuration>
]]>
    </programlisting>
    </section>

    <section><title><filename>regionservers</filename></title>
    <para>In this file you list the nodes that will run regionservers.  In
    our case we run regionservers on all but the head node example1 which is
    carrying the HBase master and the HDFS namenode</para>
    <programlisting>
    example1
    example3
    example4
    example5
    example6
    example7
    example8
    example9
    </programlisting>
    </section>

    <section><title><filename>hbase-env.sh</filename></title>
    <para>Below we use a <command>diff</command> to show the differences from 
    default in the <filename>hbase-env.sh</filename> file. Here we are setting
the HBase heap to be 4G instead of the default 1G.
    </para>
    <programlisting>
    <![CDATA[
$ git diff hbase-env.sh
diff --git a/conf/hbase-env.sh b/conf/hbase-env.sh
index e70ebc6..96f8c27 100644
--- a/conf/hbase-env.sh
+++ b/conf/hbase-env.sh
@@ -31,7 +31,7 @@ export JAVA_HOME=/usr/lib//jvm/java-6-sun/
 # export HBASE_CLASSPATH=
 
 # The maximum amount of heap to use, in MB. Default is 1000.
-# export HBASE_HEAPSIZE=1000
+export HBASE_HEAPSIZE=4096
 
 # Extra Java runtime options.
 # Below are what we set by default.  May only work with SUN JVM.
]]>
    </programlisting>
    </section>

    </section>
    
    </section>
    </section>
  </chapter>

  <chapter xml:id="configuration">
    <title>Configuration</title>
    <para>
        HBase uses the same configuration system as Hadoop.
        To configure a deploy, edit a file of environment variables
        in <filename>conf/hbase-env.sh</filename> -- this configuration
        is used mostly by the launcher shell scripts getting the cluster
        off the ground -- and then add configuration to an xml file to
        do things like override HBase defaults, tell HBase what Filesystem to
        use, and the location of the ZooKeeper ensemble.
    </para>

    <section>
    <title><filename>hbase-site.xml</filename> and <filename>hbase-default.xml</filename></title>
    <para>What are these?
    </para>
    <para>
    Not all configuration options make it out to
    <filename>hbase-default.xml</filename>.  Configuration
    that it thought rare anyone would change can exist only
    in code; the only way to turn up the configurations is
    via a reading of the source code.
    </para>
    <!--The file hbase-default.xml is generated as part of
    the build of the hbase site.  See the hbase pom.xml.
    The generated file is a docbook section with a glossary
    in it-->
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
      href="../../target/site/hbase-default.xml" />
    </section>

      <section>
      <title><filename>hbase-env.sh</filename></title>
      <para></para>
      </section>

      <section>
      <title><filename>log4j.properties</filename></title>
      <para></para>
      </section>

      <section xml:id="important_configurations">
      <title>The Important Configurations</title>
      <para>Below we list the important Configurations.  We've divided this section into
      required configuration and worth-a-look recommended configs.
      </para>


      <section xml:id="required_configuration"><title>Required Configurations</title>
      <para>See the <link linkend="requirements">Requirements</link> section.
      It lists at least two required configurations needed running HBase bearing
      load: i.e. <link linkend="ulimit">file descriptors <varname>ulimit</varname></link> and
      <link linkend="dfs.datanode.max.xcievers"><varname>dfs.datanode.max.xcievers</varname></link>.
      </para>
      </section>

      <section xml:id="recommended_configurations"><title>Recommended Configuations</title>
      <section xml:id="lzo">
      <title>LZO compression</title>
      <para>You should consider enabling LZO compression.  Its
      near-frictionless and in most all cases boosts performance.
      </para>
      <para>Unfortunately, HBase cannot ship with LZO because of
      the licensing issues; HBase is Apache-licensed, LZO is GPL.
      Therefore LZO install is to be done post-HBase install.
      See the <link xlink:href="http://wiki.apache.org/hadoop/UsingLzoCompression">Using LZO Compression</link>
      wiki page for how to make LZO work with HBase.
      </para>
      <para>A common problem users run into when using LZO is that while initial
      setup of the cluster runs smooth, a month goes by and some sysadmin goes to
      add a machine to the cluster only they'll have forgotten to do the LZO
      fixup on the new machine.  In versions since HBase 0.90.0, we should
      fail in a way that makes it plain what the problem is, but maybe not.
      Remember you read this paragraph<footnote><para>See
      <link linkend="hbase.regionserver.codec">hbase.regionserver.codec</link>
      for a feature to help protect against failed LZO install</para></footnote>.
      </para>
      </section>
      </section>

      </section>
  </chapter>

  <chapter xml:id="shell">
    <title>The HBase Shell</title>

    <para>
        The HBase Shell is <link xlink:href="http://jruby.org">(J)Ruby</link>'s
        IRB with some HBase particular verbs addded.  Anything you can do in
        IRB, you should be able to do in the HBase Shell.</para>
        <para>To run the HBase shell, 
        do as follows:
        <programlisting>$ ./bin/hbase shell</programlisting>
        Type <command>help</command> followed by <command>&lt;RETURN&gt;</command>
        to see a complete listing of commands available.
        Take some time to study the tail of the help screen where it
        does a synopsis of IRB syntax specifying arguments -- usually you must
        quote -- and how to write out dictionaries, etc.
    </para>

    <section><title>Scripting</title>
        <para>For examples scripting HBase, look in the
            HBase <filename>bin</filename> directory.  Look at the files
            that end in <filename>*.rb</filename>.  To run one of these
            files, do as follows:
            <programlisting>$ ./bin/hbase org.jruby.Main PATH_TO_SCRIPT</programlisting>
        </para>
    </section>

    <section><title>Shell Tricks</title>
        <section><title><filename>irbrc</filename></title>
                <para>Create an <filename>.irbrc</filename> file for yourself in your
                    home directory. Add HBase Shell customizations. A useful one is
                    command history:
                    <programlisting>
                        $ more .irbrc
                        require 'irb/ext/save-history'
                        IRB.conf[:SAVE_HISTORY] = 100
                        IRB.conf[:HISTORY_FILE] = "#{ENV['HOME']}/.irb-save-history"
                    </programlisting>
                </para>
        </section>
        <section><title>Log data to timestamp</title>
            <para>
                To convert the date '08/08/16 20:56:29' from an hbase log into a timestamp, do:
                <programlisting>
                    hbase(main):021:0> import java.text.SimpleDateFormat
                    hbase(main):022:0> import java.text.ParsePosition
                    hbase(main):023:0> SimpleDateFormat.new("yy/MM/dd HH:mm:ss").parse("08/08/16 20:56:29", ParsePosition.new(0)).getTime() => 1218920189000
                </programlisting>
            </para>
            <para>
                To go the other direction:
                <programlisting>
                    hbase(main):021:0> import java.util.Date
                    hbase(main):022:0> Date.new(1218920189000).toString() => "Sat Aug 16 20:56:29 UTC 2008"
                </programlisting>
            </para>
            <para>
                To output in a format that is exactly like hbase log format is a pain messing with
                <link xlink:href="http://download.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html">SimpleDateFormat</link>.
            </para>
        </section>
        <section><title>Debug</title>
            <section><title>Shell debug switch</title>
                <para>You can set a debug switch in the shell to see more output
                    -- e.g. more of the stack trace on exception --
                    when you run a command:
                    <programlisting>hbase> debug &lt;RETURN&gt;</programlisting>
                 </para>
            </section>
            <section><title>DEBUG log level</title>
                <para>To enable DEBUG level logging in the shell,
                    launch it with the <command>-d</command> option.
                    <programlisting>$ ./bin/hbase shell -d</programlisting>
               </para>
            </section>
         </section>
    </section>
  </chapter>

  <chapter xml:id="datamodel">
    <title>Data Model</title>
  <para>The HBase data model resembles that a traditional RDBMS.
  Applications store data into HBase <emphasis>tables</emphasis>.
      Tables are made of rows and columns. Table cells
      -- the intersection of row and column
      coordinates -- are versioned. By default, their
      <emphasis>version</emphasis> is a timestamp
      auto-assigned by HBase at the time of cell insertion. A cellâ€™s content
      is an uninterpreted array of bytes.
  </para>
      <para>Table row keys are also byte arrays so almost anything can
      serve as a row key from strings to binary representations of longs or
      even serialized data structures. Rows in HBase tables
      are sorted by row key. The sort is byte-ordered. All table accesses are
      via the table row key -- its primary key.
</para>

    <section>
      <title>Table</title>

      <para></para>
    </section>

    <section>
      <title>Row</title>

      <para></para>
    </section>

    <section xml:id="columnfamily">
      <title>Column Family<indexterm><primary>Column Family</primary></indexterm></title>
        <para>
      Columns in HBase are grouped into <emphasis>column families</emphasis>.
      All column members of a column family have a common prefix.  For example, the
      columns <emphasis>courses:history</emphasis> and
      <emphasis>courses:math</emphasis> are both members of the
      <emphasis>courses</emphasis> column family.
          The colon character (<literal
          moreinfo="none">:</literal>) delimits the column family from the
          column family <emphasis>qualifier</emphasis>.
        The column family prefix must be composed of
      <emphasis>printable</emphasis> characters. The qualifying tail, the
      <indexterm>column family <emphasis>qualifier</emphasis><primary>Column Family Qualifier</primary></indexterm>, can be made of any
      arbitrary bytes. Column families must be declared up front
      at schema definition time whereas columns do not need to be
      defined at schema time but can be conjured on the fly while
      the table is up an running.</para>
      <para>Physically, all column family members are stored together on the
      filesystem.  Because tunings and
      storage specifications are done at the column family level, it is
      advised that all column family members have the same general access
      pattern and size characteristics.</para>

      <para></para>
    </section>

    <section xml:id="versions">
      <title>Versions<indexterm><primary>Versions</primary></indexterm></title>

      <para>A <emphasis>{row, column, version} </emphasis>tuple exactly
      specifies a <literal>cell</literal> in HBase. Its possible to have an
      unbounded number of cells where the row and column are the same but the
      cell address differs only in its version dimension.</para>

      <para>While rows and column keys are expressed as bytes, the version is
      specified using a long integer. Typically this long contains time
      instances such as those returned by
      <code>java.util.Date.getTime()</code> or
      <code>System.currentTimeMillis()</code>, that is: <quote>the difference,
      measured in milliseconds, between the current time and midnight, January
      1, 1970 UTC</quote>.</para>

      <para>The HBase version dimension is stored in decreasing order, so that
      when reading from a store file, the most recent values are found
      first.</para>

      <para>There is a lot of confusion over the semantics of
      <literal>cell</literal> versions, in HBase. In particular, a couple
      questions that often come up are:<itemizedlist>
          <listitem>
            <para>If multiple writes to a cell have the same version, are all
            versions maintained or just the last?<footnote>
                <para>Currently, only the last written is fetchable.</para>
              </footnote></para>
          </listitem>

          <listitem>
            <para>Is it OK to write cells in a non-increasing version
            order?<footnote>
                <para>Yes</para>
              </footnote></para>
          </listitem>
        </itemizedlist></para>

      <para>Below we describe how the version dimension in HBase currently
      works<footnote>
          <para>See <link
          xlink:href="https://issues.apache.org/jira/browse/HBASE-2406">HBASE-2406</link>
          for discussion of HBase versions. <link
          xlink:href="http://outerthought.org/blog/417-ot.html">Bending time
          in HBase</link> makes for a good read on the version, or time,
          dimension in HBase. It has more detail on versioning than is
          provided here. As of this writing, the limiitation
          <emphasis>Overwriting values at existing timestamps</emphasis>
          mentioned in the article no longer holds in HBase. This section is
          basically a synopsis of this article by Bruno Dumon.</para>
        </footnote>.</para>

      <section>
        <title>Versions and HBase Operations</title>

        <para>In this section we look at the behavior of the version dimension
        for each of the core HBase operations.</para>

        <section>
          <title>Get/Scan</title>

          <para>Gets are implemented on top of Scans. The below discussion of
          Get applies equally to Scans.</para>

          <para>By default, i.e. if you specify no explicit version, when
          doing a <literal>get</literal>, the cell whose version has the
          largest value is returned (which may or may not be the latest one
          written, see later). The default behavior can be modified in the
          following ways:</para>

          <itemizedlist>
            <listitem>
              <para>to return more than one version, see <link
              xlink:href="http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/client/Get.html#setMaxVersions()">Get.setMaxVersions()</link></para>
            </listitem>

            <listitem>
              <para>to return versions other than the latest, see <link
              xlink:href="???">Get.setTimeRange()</link></para>

              <para>To retrieve the latest version that is less than or equal
              to a given value, thus giving the 'latest' state of the record
              at a certain point in time, just use a range from 0 to the
              desired version and set the max versions to 1.</para>
            </listitem>
          </itemizedlist>
        </section>

        <section>
          <title>Put</title>

          <para>Doing a put always creates a new version of a
          <literal>cell</literal>, at a certain timestamp. By default the
          system uses the server's <literal>currentTimeMillis</literal>, but
          you can specify the version (= the long integer) yourself, on a
          per-column level. This means you could assign a time in the past or
          the future, or use the long value for non-time purposes.</para>

          <para>To overwrite an existing value, do a put at exactly the same
          row, column, and version as that of the cell you would
          overshadow.</para>
        </section>

        <section>
          <title>Delete</title>

          <para>When performing a delete operation in HBase, there are two
          ways to specify the versions to be deleted</para>

          <itemizedlist>
            <listitem>
              <para>Delete all versions older than a certain timestamp</para>
            </listitem>

            <listitem>
              <para>Delete the version at a specific timestamp</para>
            </listitem>
          </itemizedlist>

          <para>A delete can apply to a complete row, a complete column
          family, or to just one column. It is only in the last case that you
          can delete explicit versions. For the deletion of a row or all the
          columns within a family, it always works by deleting all cells older
          than a certain version.</para>

          <para>Deletes work by creating <emphasis>tombstone</emphasis>
          markers. For example, let's suppose we want to delete a row. For
          this you can specify a version, or else by default the
          <literal>currentTimeMillis</literal> is used. What this means is
          <quote>delete all cells where the version is less than or equal to
          this version</quote>. HBase never modifies data in place, so for
          example a delete will not immediately delete (or mark as deleted)
          the entries in the storage file that correspond to the delete
          condition. Rather, a so-called <emphasis>tombstone</emphasis> is
          written, which will mask the deleted values<footnote>
              <para>When HBase does a major compaction, the tombstones are
              processed to actually remove the dead values, together with the
              tombstones themselves.</para>
            </footnote>. If the version you specified when deleting a row is
          larger than the version of any value in the row, then you can
          consider the complete row to be deleted.</para>
        </section>
      </section>

      <section>
        <title>Current Limitations</title>

        <para>There are still some bugs (or at least 'undecided behavior')
        with the version dimension that will be addressed by later HBase
        releases.</para>

        <section>
          <title>Deletes mask Puts</title>

          <para>Deletes mask puts, even puts that happened after the delete
          was entered<footnote>
              <para><link
              xlink:href="https://issues.apache.org/jira/browse/HBASE-2256">HBASE-2256</link></para>
            </footnote>. Remember that a delete writes a tombstone, which only
          disappears after then next major compaction has run. Suppose you do
          a delete of everything &lt;= T. After this you do a new put with a
          timestamp &lt;= T. This put, even if it happened after the delete,
          will be masked by the delete tombstone. Performing the put will not
          fail, but when you do a get you will notice the put did have no
          effect. It will start working again after the major compaction has
          run. These issues should not be a problem if you use
          always-increasing versions for new puts to a row. But they can occur
          even if you do not care about time: just do delete and put
          immediately after each other, and there is some chance they happen
          within the same millisecond.</para>
        </section>

        <section>
          <title>Major compactions change query results</title>

          <para><quote>...create three cell versions at t1, t2 and t3, with a
          maximum-versions setting of 2. So when getting all versions, only
          the values at t2 and t3 will be returned. But if you delete the
          version at t2 or t3, the one at t1 will appear again. Obviously,
          once a major compaction has run, such behavior will not be the case
          anymore...<footnote>
              <para>See <emphasis>Garbage Collection</emphasis> in <link
              xlink:href="http://outerthought.org/blog/417-ot.html">Bending
              time in HBase</link> </para>
            </footnote></quote></para>
        </section>
      </section>
    </section>
  </chapter>


  <chapter xml:id="filesystem">
    <title>Filesystem Format</title>

    <subtitle>How HBase is persisted on the Filesystem</subtitle>

    <section>
      <title>HFile</title>

      <section xml:id="hfile_tool">
        <title>HFile Tool</title>

        <para>To view a textualized version of hfile content, you can do use
        the <classname>org.apache.hadoop.hbase.io.hfile.HFile
        </classname>tool. Type the following to see usage:<programlisting><code>$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile </code> </programlisting>For
        example, to view the content of the file
        <filename>hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475</filename>,
        type the following:<programlisting> <code>$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475 </code> </programlisting>If
        you leave off the option -v to see just a summary on the hfile. See
        usage for other things to do with the <classname>HFile</classname>
        tool.</para>
      </section>
    </section>
  </chapter>

  <chapter xml:id="architecture">
    <title>Architecture</title>
    <section>
    <title>Regions</title>

    <para>This chapter is all about Regions.</para>

    <note>
      <para>Does this belong in the data model chapter?</para>
    </note>

    <section>
      <title>Region Size</title>

      <para>Region size is one of those tricky things, there are a few factors
      to consider:</para>

      <itemizedlist>
        <listitem>
          <para>Regions are the basic element of availability and
          distribution.</para>
        </listitem>

        <listitem>
          <para>HBase scales by having regions across many servers. Thus if
          you have 2 regions for 16GB data, on a 20 node machine you are a net
          loss there.</para>
        </listitem>

        <listitem>
          <para>High region count has been known to make things slow, this is
          getting better, but it is probably better to have 700 regions than
          3000 for the same amount of data.</para>
        </listitem>

        <listitem>
          <para>Low region count prevents parallel scalability as per point
          #2. This really cant be stressed enough, since a common problem is
          loading 200MB data into HBase then wondering why your awesome 10
          node cluster is mostly idle.</para>
        </listitem>

        <listitem>
          <para>There is not much memory footprint difference between 1 region
          and 10 in terms of indexes, etc, held by the regionserver.</para>
        </listitem>
      </itemizedlist>

      <para>Its probably best to stick to the default, perhaps going smaller
      for hot tables (or manually split hot regions to spread the load over
      the cluster), or go with a 1GB region size if your cell sizes tend to be
      largish (100k and up).</para>
    </section>

    <section>
      <title>Region Transitions</title>

      <note>
        <para>TODO: Review all of the below to ensure it matches what was
        committed -- St.Ack 20100901</para>
      </note>

      <para>Regions only transition in a limited set of circumstances.</para>

      <section>
        <title>Cluster Startup</title>

        <para>During cluster startup, the Master will know that it is a
        cluster startup and do a bulk assignment.</para>

        <note>
          <para>This should take HDFS block locations into account.</para>
        </note>

        <itemizedlist>
          <listitem>
            <para>Master startup determines whether this is startup or
            failover by counting the number of RegionServer nodes in
            ZooKeeper.</para>
          </listitem>

          <listitem>
            <para>Master waits for the minimum number of RegionServers to be
            available to be assigned regions.</para>
          </listitem>

          <listitem>
            <para>Master clears out anything in the
            <filename>/unassigned</filename> directory in ZooKeeper.</para>
          </listitem>

          <listitem>
            <para>Master randomly assigns out <constant>-ROOT-</constant> and
            then <constant>.META.</constant>.</para>
          </listitem>

          <listitem>
            <para>Master determines a bulk assignment plan via the
            <classname>LoadBalancer</classname></para>
          </listitem>

          <listitem>
            <para>Master stores the plan in the
            <classname>AssignmentManager</classname>.</para>
          </listitem>

          <listitem>
            <para>Master creates <code>OFFLINE</code> ZooKeeper nodes in
            <filename>/unassigned</filename> for every region.</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to each RegionServer, telling them to
            <code>OPEN</code> their regions.</para>
          </listitem>
        </itemizedlist>

        <para>All special cluster startup logic ends here.</para>

        <note>
          <para>So what can go wrong?</para>

          <itemizedlist>
            <listitem>
              <para>We assume that the Master will not fail until after the
              <code>OFFLINE</code> nodes have been created in ZK.
              RegionServers can fail at any time.</para>
            </listitem>

            <listitem>
              <para>If an RS fails at some point during this process, normal
              region open/opening/opened handling will take care of it.</para>

              <para>If the RS successfully opened a region, then it will be
              taken care of in the normal RS failure handling.</para>

              <para>If the RS did not successfully open a region, the
              RegionManager or MasterPlanner will notice that the OFFLINE (or
              OPENING) node in ZK has not been updated. This will trigger a
              re-assignment to a different server. This logic is not special
              to startup, all assignments will eventually time out if the
              destination server never proceeds.</para>
            </listitem>

            <listitem>
              <para>If the Master fails (after creating the ZK nodes), the
              failed-over Master will see all of the regions in transition. It
              will handle it in the same way any failed-over Master will
              handle existing regions in transition.</para>
            </listitem>
          </itemizedlist>
        </note>
      </section>

      <section>
        <title>Load Balancing</title>

        <para>Periodically, and when there are not any regions in transition,
        a load balancer will run and move regions around to balance cluster
        load.</para>

        <itemizedlist>
          <listitem>
            <para>Periodic timer expires initializing a load balance (Load
            Balancer is an instance of <classname>Chore</classname>).</para>
          </listitem>

          <listitem>
            <para>Currently if regions in transition, load balancer goes back
            to sleep.</para>

            <note>
              <para>Should it block until there are no regions in
              transition.</para>
            </note>
          </listitem>

          <listitem>
            <para>The <classname>AssignmentManager</classname> determines a
            balancing plan via the LoadBalancer.</para>
          </listitem>

          <listitem>
            <para>Master stores the plan in the
            <classname>AssignmentMaster</classname> store of
            <classname>RegionPlan</classname>s</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to the source RSs, telling them to
            <code>CLOSE</code> the regions.</para>
          </listitem>
        </itemizedlist>

        <para>That is it for the initial part of the load balance. Further
        steps will be executed following event-triggers from ZK or timeouts if
        closes run too long. It's not clear what to do in the case of a
        long-running CLOSE besides ask again.</para>

        <itemizedlist>
          <listitem>
            <para>RS receives CLOSE RPC, changes to CLOSING, and begins
            closing the region.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now CLOSING but does
            nothing.</para>
          </listitem>

          <listitem>
            <para>RS closes region and changes ZK node to CLOSED.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now CLOSED.</para>
          </listitem>

          <listitem>
            <para>Master looks at the plan for the specified region to figure
            out the desired destination server.</para>
          </listitem>

          <listitem>
            <para>Master sends an RPC to the destination RS telling it to OPEN
            the region.</para>
          </listitem>

          <listitem>
            <para>RS receives OPEN RPC, changes to OPENING, and begins opening
            the region.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now OPENING but does
            nothing.</para>
          </listitem>

          <listitem>
            <para>RS opens region and changes ZK node to OPENED. Edits .META.
            updating the regions location.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now OPENED.</para>
          </listitem>

          <listitem>
            <para>Master removes the region from all in-memory
            structures.</para>
          </listitem>

          <listitem>
            <para>Master deletes the OPENED node from ZK.</para>
          </listitem>
        </itemizedlist>

        <para>The Master or RSs can fail during this process. There is nothing
        special about handling regions in transition due to load balancing so
        consult the descriptions below for how this is handled.</para>
      </section>

      <section>
        <title>Table Enable/Disable</title>

        <para>Users can enable and disable tables manually. This is done to
        make config changes to tables, drop tables, etc...</para>

        <note>
          <para>Because all failover logic is designed to ensure assignment of
          all regions in transition, these operations will not properly ride
          over Master or RegionServer failures. Since these are
          client-triggered operations, this should be okay for the initial
          master design. Moving forward, a special node could be put in ZK to
          denote that a enable/disable has been requested. Another option is
          to persist region movement plans into ZK instead of just in-memory.
          In that case, an empty destination would signal that the region
          should not be reopened after being closed.</para>
        </note>

        <section>
          <title>Disable</title>

          <itemizedlist>
            <listitem>
              <para>Client sends Master an RPC to disable a table.</para>
            </listitem>

            <listitem>
              <para>Master finds all regions of the table.</para>
            </listitem>

            <listitem>
              <para>Master stores the plan (do not re-open the regions once
              closed).</para>
            </listitem>

            <listitem>
              <para>Master sends RPCs to RSs to close all the regions of the
              table.</para>
            </listitem>

            <listitem>
              <para>RS receives CLOSE RPC, creates ZK node in CLOSING state,
              and begins closing the region.</para>
            </listitem>

            <listitem>
              <para>Master sees that region is now CLOSING but does
              nothing.</para>
            </listitem>

            <listitem>
              <para>RS closes region and changes ZK node to CLOSED.</para>
            </listitem>

            <listitem>
              <para>Master sees that region is now CLOSED.</para>
            </listitem>

            <listitem>
              <para>Master looks at the plan for the specified region and sees
              that it should not reopen.</para>
            </listitem>

            <listitem>
              <para>Master deletes the unassigned znode. It is no longer
              responsible for ensuring assignment/availability of this
              region.</para>
            </listitem>
          </itemizedlist>

          <section>
            <title>Enable</title>

            <itemizedlist>
              <listitem>
                <para>Client sends Master an RPC to disable a table.</para>
              </listitem>

              <listitem>
                <para>Master finds all regions of the table.</para>
              </listitem>

              <listitem>
                <para>Master creates an unassigned node in an OFFLINE state
                for each region.</para>
              </listitem>

              <listitem>
                <para>Master sends RPCs to RSs to open all the regions of the
                table.</para>
              </listitem>

              <listitem>
                <para>RS receives OPEN RPC, transitions ZK node to OPENING
                state, and begins opening the region.</para>
              </listitem>

              <listitem>
                <para>Master sees that region is now OPENING but does
                nothing.</para>
              </listitem>

              <listitem>
                <para>RS opens region and changes ZK node to OPENED.</para>
              </listitem>

              <listitem>
                <para>Master sees that region is now OPENED.</para>
              </listitem>

              <listitem>
                <para>Master deletes the unassigned znode.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>
      </section>

      <section>
        <title>RegionServer Failure</title>

        <itemizedlist>
          <listitem>
            <para>Master is alerted via ZK that an RS ephemeral node is
            gone.</para>
          </listitem>

          <listitem>
            <para>Master begins RS failure process.</para>
          </listitem>

          <listitem>
            <para>Master determines which regions need to be handled.</para>
          </listitem>

          <listitem>
            <para>Master in-memory state shows all regions currently assigned
            to the dead RS.</para>
          </listitem>

          <listitem>
            <para>Master in-memory plans show any regions that were in
            transitioning to the dead RS.</para>
          </listitem>

          <listitem>
            <para>With list of regions, Master now forces assignment of all
            regions to other RSs.</para>
          </listitem>

          <listitem>
            <para>Master creates or force updates all existing ZK unassigned
            nodes to be OFFLINE.</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to RSs to open all the regions.</para>
          </listitem>

          <listitem>
            <para>Normal operations from here on.</para>
          </listitem>
        </itemizedlist>

        <para>There are some complexities here. For regions in transition that
        were somehow involved with the dead RS, these could be in any of the 5
        states in ZK.</para>

        <itemizedlist>
          <listitem>
            <para><code>OFFLINE</code> Generate a new assignment and send an
            OPEN RPC.</para>
          </listitem>

          <listitem>
            <para><code>CLOSING</code> If the failed RS is the source, we
            overwrite the state to OFFLINE, generate a new assignment, and
            send an OPEN RPC. If the failed RS is the destination, we
            overwrite the state to OFFLINE and send an OPEN RPC to the
            original destination. If for some reason we don't have an existing
            plan (concurrent Master failure), generate a new assignment and
            send an OPEN RPC.</para>
          </listitem>

          <listitem>
            <para><code>CLOSED</code> If the failed RS is the source, we can
            safely ignore this. The normal ZK event handling should deal with
            this. If the failed RS is the destination, we generate a new
            assignment and send an OPEN RPC.</para>
          </listitem>

          <listitem>
            <para>OPENING or OPENED If the failed RS was the original source,
            ignore. If the failed RS is the destination, we overwrite the
            state to OFFLINE, generate a new assignment, and send an OPEN
            RPC.</para>
          </listitem>
        </itemizedlist>

        <para>In all of these cases, it is important to note that the
        transitions on the RS side ensure only a single RS ever successfully
        completes a transition. This is done by reading the current state,
        verifying it is expected, and then issuing the update with the version
        number of the read value. If multiple RSs are attempting this
        operation, exactly one can succeed.</para>
      </section>

      <section>
        <title>Master Failover</title>

        <itemizedlist>
          <listitem>
            <para>Master initializes and finds out that he is a failed-over
            Master.</para>
          </listitem>

          <listitem>
            <para>Before Master starts up the normal handlers for region
            transitions he grabs all nodes in /unassigned.</para>
          </listitem>

          <listitem>
            <para>If no regions are in transition, failover is done and he
            continues.</para>
          </listitem>

          <listitem>
            <para>If regions are in transition, each will be handled according
            to the current region state in ZK.</para>
          </listitem>

          <listitem>
            <para>Before processing the regions in transition, the normal
            handlers start to ensure we don't miss any transitions. The
            handling of opens on the RS side ensures we don't dupe assign even
            if things have changed before we finish acting on
            them.<itemizedlist>
                <listitem>
                  <para>OFFLINE Generate a new assignment and send an OPEN
                  RPC.</para>
                </listitem>

                <listitem>
                  <para>CLOSING Nothing to be done. Normal handlers take care
                  of timeouts.</para>
                </listitem>

                <listitem>
                  <para>CLOSED Generate a new assignment and send an OPEN
                  RPC.</para>
                </listitem>

                <listitem>
                  <para>OPENING Nothing to be done. Normal handlers take care
                  of timeouts.</para>
                </listitem>

                <listitem>
                  <para>OPENED Delete the node from ZK. Region was
                  successfully opened but the previous Master did not
                  acknowledge it.</para>
                </listitem>
              </itemizedlist></para>
          </listitem>

          <listitem>
            <para>Once this is done, everything further is dealt with as
            normal by the RegionManager.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Summary of Region Transition States</title>

        <note>
          <para>Check below is complete -- St.Ack 20100901</para>
        </note>

        <section>
          <title>Master</title>

          <itemizedlist>
            <listitem>
              <para>Master creates an unassigned node as OFFLINE.</para>

              <para>Cluster startup and table enabling.</para>
            </listitem>

            <listitem>
              <para>Master forces an existing unassigned node to
              OFFLINE.</para>

              <para>RegionServer failure.</para>

              <para>Allows transitions from all states to OFFLINE.</para>
            </listitem>

            <listitem>
              <para>Master deletes an unassigned node that was in a OPENED
              state.</para>

              <para>Normal region transitions. Besides cluster startup, no
              other deletions of unassigned nodes is allowed.</para>
            </listitem>

            <listitem>
              <para>Master deletes all unassigned nodes regardless of
              state.</para>

              <para>Cluster startup before any assignment happens.</para>
            </listitem>
          </itemizedlist>
        </section>

        <section>
          <title>RegionServer</title>

          <itemizedlist>
            <listitem>
              <para>RegionServer creates an unassigned node as CLOSING.</para>

              <para>All region closes will do this in response to a CLOSE RPC
              from Master.</para>

              <para>A node can never be transitioned to CLOSING, only
              created.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from CLOSING
              to CLOSED.</para>

              <para>Normal region closes. CAS operation.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from OFFLINE
              to OPENING.</para>

              <para>All region opens will do this in response to an OPEN RPC
              from the Master.</para>

              <para>Normal region opens. CAS operation.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from OPENING
              to OPENED.</para>

              <para>Normal region opens. CAS operation.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>

      <section>
        <title>Region Splits</title>

        <para>Splits run unaided on the RegionServer; i.e. the Master does not
        participate. The RegionServer splits a region, offlines the split
        region and then adds the daughter regions to META, opens daughters on
        the parent's hosting RegionServer and then reports the split to the
        master.</para>
      </section>
    </section>
    </section>
  </chapter>

  <chapter>
    <title xml:id="wal">The WAL</title>

    <subtitle>HBase's<link
    xlink:href="http://en.wikipedia.org/wiki/Write-ahead_logging"> Write-Ahead
    Log</link></subtitle>

    <para>Each RegionServer adds updates to its Write-ahead Log (WAL)
    first, and then to memory.</para>

    <section>
      <title>What is the purpose of the HBase WAL</title>

      <para>The HBase WAL is...</para>
    </section>

    <section>
      <title>WAL splitting</title>

      <subtitle>How edits are recovered from a crashed RegionServer</subtitle>

      <para>When a RegionServer crashes, it will lose its ephemeral lease in
      ZooKeeper...TODO</para>

      <section>
        <title><varname>hbase.hlog.split.skip.errors</varname></title>

        <para>When set to <constant>true</constant>, the default, any error
        encountered splitting will be logged, the problematic WAL will be
        moved into the <filename>.corrupt</filename> directory under the hbase
        <varname>rootdir</varname>, and processing will continue. If set to
        <constant>false</constant>, the exception will be propagated and the
        split logged as failed.<footnote>
            <para>See <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-2958">HBASE-2958
            When hbase.hlog.split.skip.errors is set to false, we fail the
            split but thats it</link>. We need to do more than just fail split
            if this flag is set.</para>
          </footnote></para>
      </section>

      <section>
        <title>How EOFExceptions are treated when splitting a crashed
        RegionServers' WALs</title>

        <para>If we get an EOF while splitting logs, we proceed with the split
        even when <varname>hbase.hlog.split.skip.errors</varname> ==
        <constant>false</constant>. An EOF while reading the last log in the
        set of files to split is near-guaranteed since the RegionServer likely
        crashed mid-write of a record. But we'll continue even if we got an
        EOF reading other than the last file in the set.<footnote>
            <para>For background, see <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-2643">HBASE-2643
            Figure how to deal with eof splitting logs</link></para>
          </footnote></para>
      </section>
    </section>

  </chapter>

  <chapter>
    <title xml:id="blooms">Bloom Filters</title>

    <para>Bloom filters were developed over in <link
    xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200
    Add bloomfilters</link>.<footnote>
        <para>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <emphasis>Development Process</emphasis> section
        of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> attached to <link
        xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200</link>.</para>
      </footnote><footnote>
        <para>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <link
        xlink:href="http://www.one-lab.org">European Commission One-Lab
        Project 034819</link>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</para>
      </footnote></para>

    <section>
      <title>Configurations</title>

      <para>Blooms are enabled by specifying options on a column family in the
      HBase shell or in java code as specification on
      <classname>org.apache.hadoop.hbase.HColumnDescriptor</classname>.</para>

      <section>
        <title><code>HColumnDescriptor</code> option</title>

        <para>Use <code>HColumnDescriptor.setBloomFilterType(NONE | ROW |
        ROWCOL)</code> to enable blooms per Column Family. Default =
        <varname>NONE</varname> for no bloom filters. If
        <varname>ROW</varname>, the hash of the row will be added to the bloom
        on each insert. If <varname>ROWCOL</varname>, the hash of the row +
        column family + column family qualifier will be added to the bloom on
        each key insert.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.enabled</varname> global kill
        switch</title>

        <para><code>io.hfile.bloom.enabled</code> in
        <classname>Configuration</classname> serves as the kill switch in case
        something goes wrong. Default = <varname>true</varname>.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.error.rate</varname></title>

        <para><varname>io.hfile.bloom.error.rate</varname> = average false
        positive rate. Default = 1%. Decrease rate by Â½ (e.g. to .5%) == +1
        bit per bloom entry.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.max.fold</varname></title>

        <para><varname>io.hfile.bloom.max.fold</varname> = guaranteed minimum
        fold rate. Most people should leave this alone. Default = 7, or can
        collapse to at least 1/128th of original size. See the
        <emphasis>Development Process</emphasis> section of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> for more on what this option means.</para>
      </section>
    </section>

    <section>
      <title>Bloom StoreFile footprint</title>

      <para>Bloom filters add an entry to the <classname>StoreFile</classname>
      general <classname>FileInfo</classname> data structure and then two
      extra entries to the <classname>StoreFile</classname> metadata
      section.</para>

      <section>
        <title>BloomFilter in the <classname>StoreFile</classname>
        <classname>FileInfo</classname> data structure</title>

        <section>
          <title><varname>BLOOM_FILTER_TYPE</varname></title>

          <para><classname>FileInfo</classname> has a
          <varname>BLOOM_FILTER_TYPE</varname> entry which is set to
          <varname>NONE</varname>, <varname>ROW</varname> or
          <varname>ROWCOL.</varname></para>
        </section>
      </section>

      <section>
        <title>BloomFilter entries in <classname>StoreFile</classname>
        metadata</title>

        <section>
          <title><varname>BLOOM_FILTER_META</varname></title>

          <para><varname>BLOOM_FILTER_META</varname> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <classname>StoreFile.Reader</classname> load</para>
        </section>

        <section>
          <title><varname>BLOOM_FILTER_DATA</varname></title>

          <para><varname>BLOOM_FILTER_DATA</varname> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</para>
        </section>
      </section>
    </section>
  </chapter>

  <appendix xml:id="tools">
    <title >Tools</title>

    <para>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</para>
    <section xml:id="hbck">
        <title>HBase <application>hbck</application></title>
        <subtitle>An <emphasis>fsck</emphasis> for your HBase install</subtitle>
        <para>To run <application>hbck</application> against your HBase cluster run
        <programlisting>$ ./bin/hbase hbck</programlisting>
        At the end of the commands output it prints <emphasis>OK</emphasis>
        or <emphasis>INCONSISTENCY</emphasis>. If your cluster reports
        inconsistencies, pass <command>-details</command> to see more detail emitted.
        If inconsistencies, run <command>hbck</command> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <command>-fix</command> may correct the inconsistency (This latter
        is an experimental feature).
        </para>
    </section>
    <section><title>HFile Tool</title>
        <para>See <link linkend="hfile_tool" >HFile Tool</link>.</para>
    </section>
    <section xml:id="wal_tools">
      <title>WAL Tools</title>

      <section xml:id="hlog_tool">
        <title><classname>HLog</classname> tool</title>

        <para>The main method on <classname>HLog</classname> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <filename>recovered.edits</filename>. directory.</para>

        <para>You can get a textual dump of a WAL file content by doing the
        following:<programlisting> <code>$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --dump hdfs://example.org:9000/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </programlisting>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <varname>STDOUT</varname> to
        <code>/dev/null</code> and testing the program return.</para>

        <para>Similarily you can force a split of a log file directory by
        doing:<programlisting> $ ./<code>bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --split hdfs://example.org:9000/hbase/.logs/example.org,60020,1283516293161/</code></programlisting></para>
      </section>
    </section>
  </appendix>
  <appendix xml:id="compression">
    <title >Compression</title>

    <para>TODO: Compression in hbase...</para>
    <section>
    <title>
    LZO
    </title>
    <para>
    Running with LZO enabled is recommended though HBase does not ship with
    LZO because of licensing issues.  To install LZO and verify its installation
    and that its available to HBase, do the following...
    </para>
    </section>

    <section id="hbase.regionserver.codec">
    <title>
    <varname>
    hbase.regionserver.codec
    </varname>
    </title>
    <para>
    To have a RegionServer test a set of codecs and fail-to-start if any
    code is missing or misinstalled, add the configuration
    <varname>
    hbase.regionserver.codec
    </varname>
    to your <filename>hbase-site.xml</filename> with a value of
    codecs to test on startup.  For example if the 
    <varname>
    hbase.regionserver.codec
    </varname> value is <code>lzo,gz</code> and if lzo is not present
    or improperly installed, the misconfigured RegionServer will fail
    to start.
    </para>
    <para>
    Administrators might make use of this facility to guard against
    the case where a new server is added to cluster but the cluster
    requires install of a particular coded.
    </para>

    </section>
  </appendix>


  <index>
  <title>Index</title>
  </index>
</book>
