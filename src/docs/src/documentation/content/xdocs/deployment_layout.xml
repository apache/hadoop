<?xml version="1.0"?>
<!--
  Copyright 2002-2004 The Apache Software Foundation

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->

<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
          "http://forrest.apache.org/dtd/document-v20.dtd">


<document>

  <header>
    <title> 
      Hadoop Deployment Layout
    </title>
  </header>

  <body>
    <section>
      <title> Introduction </title>
      <p>
        This document describes the standard deployment layout for Hadoop.  With increased complexity and evolving Hadoop ecosystem, having standard deployment layout ensures better integration between Hadoop sub-projects.  By making the installation process easier, we can lower the barrier to entry and increase Hadoop adoption.
      </p>
    </section>

    <section> 
      <title> Packages </title>
        <p>
          We need to divide Hadoop up into packages that can be independently upgraded.  The list of packages should include:
        </p>
        <ul>
          <li>Hadoop Common - Common including the native code and required jar files.</li>
          <li>HDFS Client - HDFS jars, scripts, and shared libraries.</li>
          <li>HDFS Server - jsvc executable</li>
          <li>Yarn Client - Yarn client jars and scripts</li>
          <li>Yarn Server - Yarn server jars and scripts</li>
          <li>MapReduce - MapReduce jars, scripts, and shared libraries</li>
          <li>LZO - LZ0 codec from github.com/omally/hadoop-gpl-compression</li>
          <li>Metrics - Plugins for Chukwa and Ganglia</li>
        </ul>
        <p>Packages from other teams will include:</p>
        <ul>
          <li>Pig</li>
          <li>Hive</li>
          <li>Oozie client</li>
          <li>Oozie server</li>
          <li>Howl client</li>
          <li>Howl server</li>
        </ul>
        <p>These packages should be deployable with RPM on RedHat.  We also need a package that depends on a version of each of these packages.  In general, we can generate tarballs in the new deployment layout.</p>
        <p>Note that some packages, like Pig, which are user facing, will have 2 versions installed in a given deployment.  This will be accomplished by modifying the package name and the associated binaries to include the version number.</p>
        <p>All of the following paths are based on a prefix directory that is the root of the installation.  Our packages must support having multiple Hadoop stack installation on a computer at the same time.  For RPMs, this means that the packages must be relocatable and honor the --prefix option.</p>
     </section>

 
      <section> 
        <title> Deployment </title>
        <p>It is important to have a standard deployment that results from installing the packages regardless of the package manager.  Here are the top level directories and a sample of what would be under each.  Note that all of the packages are installed "flattened" into the prefix directory.  For compatibility reasons, we should create "share/hadoop" that matches the old HADOOP_HOME and set the HADOOP_HOME variable to that.</p>
        <source>
        $PREFIX/ bin / hadoop
               |     | mapred
               |     | pig -> pig7
               |     | pig6
               |     + pig7
               |
               + etc / hadoop / core-site.xml
               |              | hdfs-site.xml
               |              + mapred-site.xml
               |
               + include / hadoop / Pipes.hh
               |         |        + TemplateFactory.hh
               |         + hdfs.h
               |
               + lib / jni / hadoop-common / libhadoop.so.0.20.0
               |     |
               |     | libhdfs.so -> libhdfs.so.0.20.0
               |     + libhdfs.so.0.20.0
               |
               + libexec / task-controller
               |
               + man / man1 / hadoop.1
               |            | mapred.1
               |            | pig6.1
               |            + pig7.1
               |
               + share / hadoop-common 
               |       | hadoop-hdfs
               |       | hadoop-mapreduce
               |       | pig6
               |       + pig7
               |
               + sbin / hdfs-admin
               |      | mapred-admin
               |
               + src / hadoop-common
               |     | hadoop-hdfs
               |     + hadoop-mapreduce
               |
               + var / lib / data-node
                     |     + task-tracker
                     |
                     | log / hadoop-datanode
                     |     + hadoop-tasktracker
                     |
                     + run / hadoop-datanode.pid
                           + hadoop-tasktracker.pid
        </source>
        <p>Note that we must continue to honor HADOOP_CONF_DIR to override the configuration location, but that it should default to $prefix/etc.  User facing binaries and scripts go into bin.  Configuration files go into etc with multiple configuration files having a directory.  JNI shared libraries go into lib/jni/$tool since Java does not allow to specify the version of the library to load.  Libraries that aren't loaded via System.loadLibrary are placed directly under lib.  64 bit versions of the libraries for platforms that support them should be placed in lib64.  All of the architecture-independent pieces, including the jars for each tool will be placed in share/$tool.  The default location for all the run time information will be in var.  The storage will be in var/lib, the logs in var/log and the pid files in var/run.</p>
      </section>

      <section> 
        <title> Path Configurations </title>
        <p>Path can be configured at compile phase or installation phase.  For RPM, it takes advantage of the --relocate directive to allow path reconfiguration at install phase.  For Debian package, path is configured at compile phase.
        </p>
          <p>Build phase parameter:</p>
          <ul>
            <li>package.prefix - Location of package prefix (Default /usr)</li>
            <li>package.conf.dir - Location of configuration directory (Default /etc/hadoop)</li>
            <li>package.log.dir - Location of log directory (Default /var/log/hadoop)</li>
            <li>package.pid.dir - Location of pid directory (Default /var/run/hadoop)</li>
          </ul>

          <p>Install phase parameter:</p>
          <source>
          rpm -i hadoop-[version]-[rev].[arch].rpm \
              --relocate /usr=/usr/local/hadoop \
              --relocate /etc/hadoop=/usr/local/etc/hadoop \
              --relocate /var/log/hadoop=/opt/logs/hadoop \
              --relocate /var/run/hadoop=/opt/run/hadoop
          </source>
      </section>

  </body>
</document>

